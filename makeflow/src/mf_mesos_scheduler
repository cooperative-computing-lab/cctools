#!/usr/bin/env python

import os
import sys
import uuid
import time
import urllib
import urllib2
import tarfile
import logging
import threading
import json
import imp
import re

setting_module_path = os.path.join(os.getenv('CCTOOLS'), "bin", "mf_mesos_setting")
mms = imp.load_source("mf_mesos_setting", setting_module_path)

sys.path.insert(0,sys.argv[3])

from mesos.interface import Scheduler
from mesos.native import MesosSchedulerDriver
from mesos.interface import mesos_pb2

logger = logging.getLogger('mesos_scheduler')
logger.setLevel(logging.INFO)
console = logging.StreamHandler()
console.setLevel(logging.INFO)
formatter = logging.Formatter(fmt='%(asctime)s.%(msecs)03d %(name)s %(levelname)s %(message)s',datefmt='%Y/%m/%d %H:%M:%S')
console.setFormatter(formatter)
logger.addHandler(console)

FILE_TASK_INFO = "mesos_task_info"
FILE_TASK_STATE = "mesos_task_state"
SLAVE_STDERR = "mesos_slave_stderr"
MESOS_DONE_FILE = "mesos_done"

MAX_FAILED = 10
PARENT_PID = 0
    

# Makeflow mesos scheduler
class MakeflowScheduler(Scheduler):

    def __init__(self, mf_wk_dir, master_addr):
        self.mf_wk_dir = mf_wk_dir
        self.master_addr = master_addr

    # Print out the dynamic static information of mesos cluster
    def print_mesos_info(self, task_id):
        metrics_snapshot_url = "http://{0}/metrics/snapshot".format(self.master_addr)

        master_metrics = json.load(urllib2.urlopen(metrics_snapshot_url))

        cpu_offered = master_metrics['master/cpus_total'] 
        cpu_used = master_metrics['master/cpus_used']
        mem_offered = master_metrics['master/mem_total']
        mem_used = master_metrics['master/mem_used']
        disk_offered = master_metrics['master/disk_total']
        disk_used = master_metrics['master/disk_used'] 
        tasks_running = master_metrics['master/tasks_running']
        tasks_staging = master_metrics['master/tasks_staging']
        tasks_starting = master_metrics['master/tasks_starting']

        # treat running, staging and starting as running
        num_running_tasks = tasks_running + tasks_staging + tasks_starting

        logger.info("{0} cpu {1} used, {2} memory {3} used, {4} disk {5} used, {6} active tasks".format(cpu_offered, cpu_used, mem_offered, mem_used, disk_offered, disk_used, num_running_tasks))

    # Create a ExecutorInfo instance for mesos task
    def new_mesos_executor(self, mf_task, framework_id, hostname, task_cpu, task_mem, task_disk):
        executor = mesos_pb2.ExecutorInfo()
        executor.framework_id.value = framework_id
        executor.executor_id.value = str(uuid.uuid4())
        cctools_path = os.getenv('CCTOOLS')
        sh_path = os.path.join(cctools_path, 'bin', 'mf-mesos-executor')
        task_cmd_sh = "task_{0}_cmd".format(mf_task.task_id)
        task_cmd_f = open(task_cmd_sh, "w+")
        task_cmd_f.write(mf_task.cmd)
        task_cmd_f.close()
        os.chmod(task_cmd_sh, 0755)

        uri = executor.command.uris.add()
        uri.value = os.path.join(os.getcwd(), task_cmd_sh)
        uri.executable = True
        uri.extract = False

        executor.name = "{0} makeflow mesos executor".format(mf_task.task_id) 
        executor.source = "python executor"
        executor.command.value = "{0} \"./{1}\" {2} {3} {4} {5}".format(sh_path, task_cmd_sh, 
                executor.executor_id.value, executor.framework_id.value, sys.argv[3], hostname)
        
        # add $CCTOOLS as the env variables for executor command
        cctools_env = executor.command.environment.variables.add()
        cctools_env.name = "CCTOOLS"
        cctools_env.value = cctools_path

        # add input files list to the executor_info
        for fn in mf_task.inp_fns:

            logger.info("input file is: {0}".format(fn.strip(' \t\n\r')))
            uri = executor.command.uris.add()

            # if input file is a directory, compress it
            if os.path.isdir(fn):
                logger.info("Input {0} of task {1} is a directory".format(fn, mf_task.task_id))
                dir_name = os.path.dirname(fn)
                base_fn = os.path.basename(fn)
                compressed_fn = "{0}.tar.gz".format(base_fn)
                if not os.path.exists(compressed_fn):
                    logger.info("The compressed file for {0} is not exist".format(fn)) 
                    tar = tarfile.open(compressed_fn, "w:gz")
                    tar.add(base_fn)
                    tar.close()
                else:
                    logger.info("The compressed file for {0} is exist".format(fn)) 
                fn = os.path.join(dir_name, compressed_fn)
                uri.value = fn.strip(' \t\n\r')
                uri.extract = True
                # uri.cache = True
            else:
                uri.value = fn.strip(' \t\n\r')
                uri.extract = False
                uri.executable = True

        return executor
    
    # Create a TaskInfo instance
    def new_mesos_task(self, offer, task_id, task_cores, task_mem, task_disk):
        mesos_task = mesos_pb2.TaskInfo()
        mesos_task.task_id.value = task_id
        mesos_task.slave_id.value = offer.slave_id.value
        mesos_task.name = "task {0}".format(str(id))
    
        cpus = mesos_task.resources.add()
        cpus.name = "cpus"
        cpus.type = mesos_pb2.Value.SCALAR
        cpus.scalar.value = task_cores
    
        mem = mesos_task.resources.add()
        mem.name = "mem"
        mem.type = mesos_pb2.Value.SCALAR
        mem.scalar.value = task_mem
   
        disk = mesos_task.resources.add()
        disk.name = "disk"
        disk.type = mesos_pb2.Value.SCALAR
        disk.scalar.value = task_disk 
    
        return mesos_task

    def launch_mesos_task(self, driver, offer, task_id, cores, mem, disk): 

        mesos_task = self.new_mesos_task(offer, task_id, cores, mem, disk)
       
        mf_mesos_task_info = mms.tasks_info_dict[task_id] 

        # initialize a ExecutorInfo instance
        executor = self.new_mesos_executor(\
            mf_mesos_task_info, \
            offer.framework_id.value, \
            offer.hostname, \
            cores, mem, disk)

        mesos_task.executor.MergeFrom(executor)

        # TODO for version0 one executor only run with one task, in the future
        # we may wanna change the number of tasks running by one executor
        mf_mesos_executor_info = \
                mms.MfMesosExecutorInfo(\
                executor.executor_id, \
                offer.slave_id.value, offer.hostname)

        mf_mesos_executor_info.tasks.append(task_id)

        mms.executors_info_dict[executor.executor_id.value] = \
                mf_mesos_executor_info

        # combine mesos TaskInfo with ExecutorInfo
        mf_mesos_task_info.executor_id = \
                executor.executor_id.value

        mms.tasks_info_dict[task_id] \
                = mf_mesos_task_info 
        
        # create mesos task and launch it with offer 
        logger.info("Launching task {0} using offer {1} from {2}.".format(\
                        task_id, offer.id.value, offer.hostname))

        # one task is corresponding to one executor
        tasks = [mesos_task]

        driver.launchTasks(offer.id, tasks)

    def resourceOffers(self, driver, offers):
        logger.info("Received resource offers: {0} from {1}".format(\
                [o.id.value for o in offers], [o.hostname for o in offers]))
       
        idle_task = False 

        for offer in offers:
          
            offer_cpus = 0
            offer_mem = 0
            offer_disk = 0

            for resource in offer.resources:
                if resource.name == "cpus":
                    offer_cpus += resource.scalar.value
                if resource.name == "mem":
                    offer_mem += resource.scalar.value
                if resource.name == "disk":
                    offer_disk += resource.scalar.value

            logger.info("Received resource offer {0} with cpus {1}, mem: {2} and disk {3} from {4}\
                    ".format(offer.id.value, offer_cpus, offer_mem, offer_disk, offer.hostname))

            remaining_cpus = offer_cpus
            remaining_mem = offer_mem
            remaining_disk = offer_disk

            with mms.lock:
                for task_info in mms.tasks_info_dict.itervalues():

                    if (task_info.action == "submitted" or task_info.action == "resubmitted")\
                       and remaining_cpus >= task_info.cores \
                       and remaining_mem >= task_info.mem \
                       and remaining_disk >= task_info.disk:

                        idle_task = True
                        task_id = task_info.task_id
                        mms.tasks_info_dict[task_id].action = "running"

                        remaining_cpus -= task_info.cores
                        remaining_mem -= task_info.mem
                        remaining_disk -= task_info.disk
                        
                        logger.info("Task {0} will use {1} cores".format(task_id, task_info.cores))
                        logger.info("Number of remaining cores are {0}".format(remaining_cpus))

                        self.launch_mesos_task(driver, offer, task_id, task_info.cores, task_info.mem, task_info.disk)
                        break 

            #if not idle_task:
            #    driver.declineOffer(offer.id)
               
    def clean_mesos_file(self, task_id): 
        mf_task = mms.tasks_info_dict[task_id]
        #  TODO do not remove the file, if it is not exist
        # if os.path.isfile("task_{0}_cmd".format(task_id)):
        #   os.remove("task_{0}_cmd".format(task_id))
        #for fn in mf_task.inp_fns:
        #    if os.path.isdir(fn):
        #        logger.info("{0} is a directory, remove the tar.gz file".format(fn))
        #        compressed_fn = "{0}.tar.gz".format(fn)
        #        if os.path.isfile(compressed_fn):
        #            os.remove(compressed_fn)
        
    def statusUpdate(self, driver, update):
        if os.path.isfile(FILE_TASK_STATE): 
            oup_fn = open(FILE_TASK_STATE, "a", 0)
        else:
            logger.error("{0} is not created in advanced".format(FILE_TASK_STATE))
            exit(1)

        with mms.lock:
            if update.state == mesos_pb2.TASK_STAGING:
                logger.info("{0} is staging".format(update.task_id.value))
                self.print_mesos_info(update.task_id.value)

            if update.state == mesos_pb2.TASK_STARTING:
                logger.info("{0} is starting".format(update.task_id.value))
                self.print_mesos_info(update.task_id.value)

            if update.state == mesos_pb2.TASK_RUNNING:
                logger.info("{0} is running".format(update.task_id.value))
                self.print_mesos_info(update.task_id.value)

            if update.state == mesos_pb2.TASK_FINISHED:
                # get the resource usage snapshot
                logger.info("{0} is finished by executor.".format(update.task_id.value))
                self.print_mesos_info(update.task_id.value)

                stderr_msg = update.data

                if os.path.isfile(SLAVE_STDERR):
                    with open (SLAVE_STDERR, "a") as stderr_fd:
                        stderr_fd.write(stderr_msg)
                else:
                    with open (SLAVE_STDERR, "w") as stderr_fd:
                        stderr_fd.write(stderr_msg)

                message = update.message
                logger.info("Receive message {0}".format(update.message))
                message_list = message.split()

                if message_list[0].strip(' \t\n\r') == "[EXECUTOR_OUTPUT]":

                    if os.path.isfile(FILE_TASK_STATE): 
                        oup_fn = open(FILE_TASK_STATE, "a", 0)
                    else:
                        logger.error("{0} is not created in advanced".format(FILE_TASK_STATE))
                        exit(1)

                    output_file_dir = message_list[1].strip(' \t\n\r')
                    curr_task_id = message_list[3].strip(' \t\n\r')

                    output_fns = mms.tasks_info_dict[curr_task_id].oup_fns

                    for output_fn in output_fns:
                        output_file_addr = "{0}/{1}".format(output_file_dir, output_fn)
                        logger.info("The output file address is: {0}".format(\
                                output_file_addr))
                        urlretrieve_start = time.time()
                        urllib.urlretrieve(output_file_addr, output_fn)
                        urlretrieve_end = time.time()
                         

                        if os.path.exists(output_fn):
                            output_fn_size = os.stat(output_fn).st_size 
                            transfer_rate = ((output_fn_size/(urlretrieve_end - urlretrieve_start))/(1024*1024))
                            logger.info("Task {0} get output file {1} with transfer rate {2} MB/s".format(curr_task_id, \
                                output_fn, transfer_rate))
                        else:
                            logger.error("Task {0} is failed because it cannot get \
                            output file {1}".format(curr_task_id, output_fn))
                            oup_fn.write("{0},failed\n".format(curr_task_id))  
                            mms.tasks_info_dict[update.task_id.value].action = "failed"
                            self.clean_mesos_file(update.task_id.value)  
                            return 
                
                    logger.info("{0} is done".format(update.task_id.value))
                    oup_fn.write("{0},finished\n".format(curr_task_id))
                    mms.tasks_info_dict[curr_task_id].action = "finished"
                    self.clean_mesos_file(curr_task_id)

                    oup_fn.close()

            if update.state == mesos_pb2.TASK_FAILED:
                failed_task_id = update.task_id.value
                logger.info("Task {0} failed with reason code {1}.".format(failed_task_id, update.reason))
                self.print_mesos_info(update.task_id.value)

                if failed_task_id in mms.tasks_failed_time:
                    if mms.tasks_failed_time[failed_task_id] <= MAX_FAILED:
                        mms.tasks_failed_time[failed_task_id] += 1
                        mms.tasks_info_dict[failed_task_id].action = "resubmitted"
                        logger.info("Task {0} has been failed {1} times". format(failed_task_id, mms.tasks_failed_time[failed_task_id])) 
                        logger.info("Try to resubmit failed task {0}".format(failed_task_id))
                    else:
                        oup_fn.write("{0},failed\n".format(failed_task_id))
                        mms.tasks_info_dict[failed_task_id].action = "failed"
                        self.clean_mesos_file(failed_task_id)
                else:  
                    mms.tasks_failed_time[failed_task_id] = 1
                    mms.tasks_info_dict[failed_task_id].action = "resubmitted"
                    logger.info("Task {0} has been failed 1 time". format(failed_task_id)) 
                    logger.info("Try to resubmit failed task {0}".format(failed_task_id))

            if update.state == mesos_pb2.TASK_KILLED:
                oup_fn.write("{0},killed\n".format(update.task_id.value))
                mms.tasks_info_dict[update.task_id.value].action = "killed"
                logger.info("{0}".format(update.message))
                self.clean_mesos_file(update.task_id.value)
                self.print_mesos_info(update.task_id.value)

            if update.state == mesos_pb2.TASK_LOST:
                oup_fn.write("{0},lost\n".format(update.task_id.value))
                mms.tasks_info_dict[update.task_id.value].action = "lost"
                logger.error("Task {0} lost with error message: {1}".format(update.task_id.value,\
                 update.message))
                self.print_mesos_info(update.task_id.value)
                self.clean_mesos_file(update.task_id.value)

            if update.state == mesos_pb2.TASK_ERROR:
                mms.tasks_info_dict[update.task_id.value].action = "resubmitted"
                logger.info("{0} try to resubmit task {1}".format(update.message,\
                update.task_id.value))
                self.print_mesos_info(update.task_id.value)
                logger.error("Task {0} fail with error message: {1}".format(update.task_id.value,\
                update.message))
        
        oup_fn.close()

    # TODO deprecate the using of this method, since the message transmission is not reliable
    def frameworkMessage(self, driver, executorId, slaveId, message):
        logger.info("Receive message {0}".format(message))
        message_list = message.split()
        if message_list[0].strip(' \t\n\r') == "[EXECUTOR_STATE]":
            curr_executor_id = message_list[1].strip(' \t\n\r')
            curr_executor_state = message_list[2].strip(' \t\n\r')
             
            with mms.lock:
                mms.executors_info_dict[curr_executor_id].state = \
                        curr_executor_state

                # if a executor is aborted, the corresponding task is aborted
                if curr_executor_state == "aborted":
                    curr_task_id = message_list[3].strip(' \t\n\r')
                    file_task_state = open(FILE_TASK_STATE, "a+")
                    file_task_state.write("{0},{1}\n".format(curr_task_id,\
                            curr_executor_state))
                    file_task_state.close()
                    os.remove("task_{0}_cmd".format(curr_task_id))


class MakefowMonitor(threading.Thread):
  
    def __init__(self, driver):
        threading.Thread.__init__(self)
        self.driver = driver

    # Check if all tasks done
    def is_all_executor_stopped(self):
        
        with mms.lock:
            for executor_info in mms.executors_info_dict.itervalues():
                if executor_info.state == "registered":
                    return False
    
            return True

    # stop all running executors
    def stop_executors(self):
        task_action_fn = open(FILE_TASK_INFO, "r")
        lines = task_action_fn.readlines()
    
        with mms.lock:
            for line in lines:
                task_info_list = re.split(''',(?=(?:[^'"]|'[^']*'|"[^"]*")*$)''', line)
                task_id = task_info_list[0]
                task_action = task_info_list[4]
                if task_action == "aborting":
                    mf_task = mms.tasks_info_dict[task_id]

                    self.driver.sendFrameworkMessage(self, \
                            mf_task.executor_id, \
                            mms.executors_info_dict[mf_task.executor_id].slave_id, \
                            "[SCHEDULER_REQUEST] abort")
    
        task_action_fn.close()

    def stop_mesos_scheduler(self):

        # If makeflow creat "makeflow_done" file, stop the scheduler
        mf_done_fn_path = os.path.join(mms.mf_wk_dir, MESOS_DONE_FILE)

        if os.path.isfile(mf_done_fn_path):
            mf_done_fn = open(mf_done_fn_path, "r")
            mf_state = mf_done_fn.readline().strip(' \t\n\r')
            mf_done_fn.close()

            logger.info("Makeflow workflow is {0}".format(mf_state))

            if mf_state == "aborted":
                logger.info("Workflow aborted, stopping executors...")
                self.stop_executors()

                    
        else: 
            
            logger.info("batch job system exited unexpectedly")

        fn_run_tks_path = os.path.join(mms.mf_wk_dir, FILE_TASK_INFO)
        fn_finish_tks_path = os.path.join(mms.mf_wk_dir, FILE_TASK_STATE)

        #if os.path.isfile(mf_done_fn_path):
        #    os.remove(mf_done_fn_path)
        #if os.path.isfile(fn_run_tks_path):
        #    os.remove(fn_run_tks_path)
        #if os.path.isfile(fn_finish_tks_path):
        #    os.remove(fn_finish_tks_path)
        
        while(not self.is_all_executor_stopped()):
            pass

        self.driver.stop()  
    
    
    def abort_mesos_task(self, task_id):
        logger.info("Makeflow is trying to abort task {0}.".format(task_id))
       
        if mms.tasks_info_dict[task_id].action == "finished" or \
                mms.tasks_info_dict[task_id].action == "failed" or \
                mms.tasks_info_dict[task_id].action == "aborted":
                return

        with mms.lock:
            if mms.tasks_info_dict[task_id].action == "submitted":
                mms.tasks_info_dict[task_id].action = "aborted"
            if mms.tasks_info_dict[task_id].action == "running":
                py_task_id = mesos_pb2.TaskID()
                py_task_id.value = task_id 
                self.driver.killTask(py_task_id)
                mms.tasks_info_dict[task_id].action = "aborted"

        if os.path.isfile(FILE_TASK_STATE): 
            oup_fn = open(FILE_TASK_STATE, "a", 0)
        else:
            logger.error("{0} is not created in advanced".format(FILE_TASK_STATE))
            exit(1)
        
        oup_fn.write("{0},aborted\n".format(task_id))
        oup_fn.close()

    def run(self):

        while(not os.path.isfile(MESOS_DONE_FILE)):

            current_ppid = os.getppid()
           
            # The parent process (i.e. batch_job_mesos) has 
            # been killed. Stop the scheduler

            if (current_ppid != PARENT_PID):
                logger.info("The parent process exit unexpectedly")
                break; 

            task_info_fp = open(FILE_TASK_INFO, "r")
            lines = task_info_fp.readlines()

            for line in lines:
                task_info_list = re.split(''',(?=(?:[^'"]|'[^']*'|"[^"]*")*$)''', line)
                task_id = task_info_list[0].strip(" \t\n\r")
                task_cmd = task_info_list[1].strip(" \t\n\r")
                task_inp_fns = task_info_list[2].split()
                task_oup_fns = task_info_list[3].split()
                task_cores = int(task_info_list[4].strip(" \t\n\r"))
                task_mem = int(task_info_list[5].strip(" \t\n\r"))
                task_disk = int(task_info_list[6].strip(" \t\n\r"))
                task_action = task_info_list[7].strip(" \t\n\r")
             
                with mms.lock:
                    # find new tasks
                    if (task_id not in mms.tasks_info_dict):

                        mf_mesos_task_info = mms.MfMesosTaskInfo(\
                                task_id, task_cmd, task_inp_fns, task_oup_fns, \
                                task_cores, task_mem, task_disk, \
                                task_action)

                        mms.tasks_info_dict[task_id] \
                                = mf_mesos_task_info

                    # makeflow trying to abort an exist task
                    if (task_id in mms.tasks_info_dict) and\
                            (task_action == "aborting"):

                        self.abort_mesos_task(task_id)                

            task_info_fp.close()
            time.sleep(2)
       
        self.stop_mesos_scheduler() 

if __name__ == '__main__':

    PARENT_PID = os.getppid()
    logger.info("The parent pid is {0}".format(PARENT_PID))

    # make us a framework
    mms.mf_wk_dir = sys.argv[1]
    mesos_master_addr = sys.argv[2]

    # create the "task_state" and "task_info" file
    if not os.path.isfile(FILE_TASK_STATE):
        open(FILE_TASK_STATE, 'w').close()
    if not os.path.isfile(FILE_TASK_INFO):
        open(FILE_TASK_INFO, 'w').close()

    # initialize a framework instance
    framework = mesos_pb2.FrameworkInfo()
    framework.user = ""  # Have Mesos fill in the current user.
    framework.name = "Makeflow"

    driver = MesosSchedulerDriver(
        MakeflowScheduler(mms.mf_wk_dir, mesos_master_addr),
        framework,
        mesos_master_addr  # assumes running on the master
    )

    # Start the monitor thread 
    mf_monitor = MakefowMonitor(driver)
    mf_monitor.start()

    status = 0 if driver.run() == mesos_pb2.DRIVER_STOPPED else 1

    sys.exit(status)
