<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>

<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8">
<link rel="stylesheet" type="text/css" href="manual.css">
<title>Makeflow User's Manual</title>
</head>

<body>

<div id="manual">
<h1>Makeflow User's Manual</h1>

<p>Makeflow is Copyright (C) 2017- The University of Notre Dame. This software is distributed under the GNU General Public License. See the file COPYING for details.</p>

<h2>Table of Contents</h2>

<ul>
<li><a href=#introduction>Introduction</a>
<ul>
<li><a href=#overview>Overview</a>
<li><a href=#installing>Installing</a>
<li><a href=#basic>Basic Usage</a>
<li><a href=#jx>JX Language</a>
<li><a href=#monitoring>Monitoring</a>
<li><a href=#resources>Resources</a>
<li><a href=#advice>General Advice</a>
</ul>

<li><a href=#batch>Batch Systems</a>
<ul>
<li><a href=#local>Local</a>
<li><a href=#htcondor>HTCondor</a>
<li><a href=#sge>SGE/OGE/UGE</a>
<li><a href=#pbs>PBS</a>
<li><a href=#torque>Torque</a>
<li><a href=#slurm>SLURM</a>
<li><a href=#moab>Moab</a>
<li><a href=#mesos>Mesos</a>
<li><a href=#ec2>Amazon EC2</a>
<li><a href=#ec2>Amazon Lambda</a>
<li><a href=#generic>Generic Cluster</a>
</ul>

<li><a href=#workqueue>Using Work Queue</a>
<ul>
<li><a href=#wqoverview>Overview</a>
<li><a href=#ports>Port Numbers</a>
<li><a href=#names>Project Names</a>
<li><a href=#password>Setting a Password</a>
</ul>

<li><a href=#container>Container Environments</a>
<ul>
<li><a href=#docker>Docker</a>
<li><a href=#singularity>Singularity</a>
<li><a href=#umbrella>Umbrella</a>
<li><a href=#wrapper>Wrapper Commands</a>
</ul>

<li><a href=#mpi>Running in an MPI Environment</a>
<ul>
<li><a href=#makeflow-mpi>Running Makeflow as an MPI Job</a>
<li><a href=#mpi-starter>Running Makeflow and WorkQueue as an MPI Job</a>
</ul>

<li><a href=#advanced>Advanced Features</a>
<ul>
<li><a href=#shared>Shared File Systems</a>
<li><a href=#consistency>NFS Consistency Delay</a>
<li><a href=#mounting>Mounting Remote Files</a>
<li><a href=#garbage>Garbage Collection</a>
<li><a href=#viz>Visualization</a>
<li><a href=#linking>Linking Dependencies</a>
<li><a href=#archiving>Archiving Jobs</a>
</ul>

<li><a href=#reference>Technical Reference</a>
<ul>
<li><a href=#language>Language Reference</a>
<li><a href=#rescat>Resources and Categories</a>
<li><a href=#log>Transaction Log</a>
</ul>

</ul>

<a name=introduction><h2>Introduction</h2></a>

<a name=overview><h3>Overview</h3></a>

<p>Makeflow is a <b>workflow engine</b> for large scale distributed computing.
It accepts a specification of a large amount of work to be performed,
and runs it on
remote machines in parallel where possible.  In addition, Makeflow is
fault-tolerant, so you can use it to coordinate very large tasks that may run
for days or weeks in the face of failures.  Makeflow is designed to be similar
to <b>Make</b>, so if you can write a Makefile, then you can write a
Makeflow.</p>

<p>Makeflow makes it easy to move a large amount of work from one facility to another.  After writing a workflow, you can test it out on your local laptop,
then run it at your university computing center, move it over to a national
computing facility like <a href=http://www.xsede.org>XSEDE</a>, and
then again to a commercial cloud system.  Using the (bundled) Work Queue system, you can
even run across multiple systems simultaneously.  No matter where you run your tasks, the workflow language stays the same.</p>

<p>Makeflow is used in production to support large scale problems
in science and engineering.  Researchers in fields such as bioinformatics,
biometrics, geography, and high energy physics all use Makeflow to compose
workflows from existing applications.</p>

Makeflow can send your jobs to a wide variety of services,
such as batch systems (HTCondor, SGE, PBS, Torque),
cluster managers (Mesos and Kubernetes), cloud services (Amazon EC2 or Lambda)
and container environments like Docker and Singularity.
Details for each of those systems are given in the <a href=#batch>Batch System Support</a> section.

<a name=installing><h3>Installing</h3></a>

<p> Makeflow is part of the <a href="http://ccl.cse.nd.edu/software">Cooperating Computing Tools</a>, which is easy to install.
In most cases, you can just build from source and install in your home directory like this:

<code>git clone https://github.com/cooperative-computing-lab/cctools cctools-source
cd cctools-source
./configure
make
make install
</code>

Then, set your path to include the appropriate directory.  If you use <tt>bash</tt>, do this:

<code>export PATH=${HOME}/cctools/bin:$PATH
</code>

Or if you use <tt>tcsh</tt>, do this:

<code>setenv PATH ${HOME}/cctools/bin:$PATH
</code>

For more complex situations, consult the <a href="install.html">CCTools installation instructions</a>.

<a name=basic><h3>Basic Usage</h3></a>

<p>A Makeflow workflow consists of a set of rules.
Each rule specifies a set of <i>output files</i> to create,
a set of <i>input files</i> needed to create them, and a <i>command</i> that
generates the target files from the source files.</p>

<p>Makeflow attempts to generate all of the output files in a workflow.  It
examines all of the rules and determines which rules must run before others.
Where possible, it runs commands in parallel to reduce the execution time.</p>

<p>Makeflow suppose two ways of writing a workflow: classic Make and JX.
Classic Make is very easy to learn and get started, but 
but can very verbose when writing large workflows.
The <a href=jx-tutorial.html>JX workflow language</a>
is a little more complex, but allows for more programmable construction
of workflows.</p>

<p>Here is an example workflow written in the classic Make language.
It uses the <tt>convert</tt> utility to make an
animation.  It downloads an image from the web, creates four variations of the
image, and then combines them back together into an animation.  The first and
the last task are marked as LOCAL to force them to run on the controlling
machine.</p>

<code>CURL=/usr/bin/curl
CONVERT=/usr/bin/convert
URL=http://ccl.cse.nd.edu/images/capitol.jpg

capitol.anim.gif: capitol.jpg capitol.90.jpg capitol.180.jpg capitol.270.jpg capitol.360.jpg $CONVERT
        LOCAL $CONVERT -delay 10 -loop 0 capitol.jpg capitol.90.jpg capitol.180.jpg capitol.270.jpg capitol.360.jpg capitol.270.jpg capitol.180.jpg capitol.90.jpg capitol.anim.gif

capitol.90.jpg: capitol.jpg $CONVERT
        $CONVERT -swirl 90 capitol.jpg capitol.90.jpg

capitol.180.jpg: capitol.jpg $CONVERT
        $CONVERT -swirl 180 capitol.jpg capitol.180.jpg

capitol.270.jpg: capitol.jpg $CONVERT
        $CONVERT -swirl 270 capitol.jpg capitol.270.jpg

capitol.360.jpg: capitol.jpg $CONVERT
        $CONVERT -swirl 360 capitol.jpg capitol.360.jpg

capitol.jpg: $CURL
        LOCAL $CURL -o capitol.jpg $URL
</code>

<p>
(Note that Makeflow differs from Make in a few subtle ways,
you can learn about those in the <a href=#reference>Language Reference</a> below.)

<p>To try out the example above, copy and paste it into a file named
<tt>example.makeflow</tt>.  To run it on your local machine:</p>

<code>makeflow example.makeflow</code>

<p>Note that if you run it a second time, nothing will happen, because all of
the files are built:</p>

<code>makeflow example.makeflow
makeflow: nothing left to do
</code>

<p>Use the <tt>--clean</tt> option to clean everything up before trying it again:</p>

<code>makeflow --clean example.makeflow</code>

<p>If you have access to a batch system like Condor, SGE, or Torque,
or a cloud service provider like Amazon, you can direct Makeflow to run your jobs there by using the <tt>-T</tt> option:</p>

<code>makeflow -T condor example.makeflow
  or
makeflow -T sge example.makeflow
  or
makeflow -T torque example.makeflow
  or
makeflow -T amazon example.makeflow
...
</code>

To learn more about the various batch system options, see the <a href=#batch>Batch System Support</a> section.

<a name=jx><h3>JX Language</h3></a>

<p>
The classic make language is easy to learn and suitable for many purposes,
but it can get rather verbose for complex workflows.  Makeflow also supports
the JX workflow language for expressing workflows in a more programmable
way.  To give you an idea, here is how to quickly generate one thousand
simulation jobs using JX:
</p>

<code>
{
  "rules": [
    {
      "command": "./simulate.py -n "+N+" > output."+N+".txt",
      "inputs": [ "simulate.py" ],
      "outputs": [ "output."+N+".txt" ],
    } for N in range(1,1000)
  ]
}
</code>

You can use the JX language with Makeflow by simply using the <tt>--jx</tt> argument to any invocation.  For example:

<code>makeflow --jx example.jx -T condor
</code>

To learn more about JX, please see:

<dir>
<li> <a href=jx-quick.html>JX Quick Reference</a>
<li> <a href=jx-tutorial.html>JX Tutorial</a>
<li> <a href=jx.html>JX Language Reference</a>
</dir>

<a name=resources><h3>Resources</h3></a>

<p>
Most batch systems require information about what resources
each job needs, so as to schedule them appropriately.  You can convey
this by setting the variables CORES, MEMORY (in MB), and DISK (in MB)
ahead of each job.  Makeflow will translate this information as needed
to the underlying batch system.  For example:
</p>

<code>CORES=4
MEMORY=1024
DISK=4000

output.txt: input.dat
    analyze input.dat > output.txt
</code>

<a name=monitoring><h3>Monitoring</h3></a>

A variety of tools are available to help you monitor the progress of a workflow as it runs.
Makeflow itself creates a transaction log (<tt>example.makeflow.makeflowlog</tt>) which
contains details of each task as it runs, tracking how many are idle,
running, complete, and so forth.  These tools can read the transaction
log and summarize the workflow:

<ol>
<li> <tt>makeflow_monitor</tt> reads the transaction log and produceds a continuous
display that shows the overall time and progress through the workflow:
<code>makeflow example.makeflow & makeflow_monitor example.makeflow.makeflowlog
</code>
<li><tt>makeflow_graph_log</tt> will read the transaction log, and produce a timeline
graph showing the number of jobs ready, running, and complete over time:
<code>makeflow example.makeflow & makeflow_graph_log example.makeflow.makeflowlog example.png
</code>
<li><tt>makeflow_viz</tt> will display the workflow in graphical form,
so that you can more easily understand the structure and dependencies.
(Read more about <a href=#visualization>Visualization</a>)
</ol>

<p>
In addition, if you give the workflow a "project name" with the <tt>-N</tt>
option, it will report its status to the <a href=catalog.html>catalog server</a> once per minute.  The <tt>makeflow_status</tt> command will query the catalog
and summarize your currently running workloads, like this:
</p>

<pre>
OWNER      PROJECT              JOBS   WAIT    RUN   COMP   ABRT   FAIL   TYPE
alfred     simulation           2263   2258      1      4      0      0 condor
betty      analysis             2260      1      1   2258      0      0     wq
</pre>

<a name=advice><h3>General Advice</h3></a>

<p>
A few key bits of advice address the most common problems encountered when using Makeflow:
</p>

<p>
First, Makeflow works best when it has accurate information about each task that you wish
to run.  Make sure that you are careful to indicate exactly which input files each task
needs, and which output files it produces.
</p>

<p>
Second, if Makeflow is doing something unexpected, you may find it useful to turn on
the debugging stream with the <tt>-d all</tt> option.  This will emit all sorts of detail
about how each job is constructed and sent to the underlying batch system. 
</p>

<p>
When debugging failed tasks, it is often useful to examine any output produced.
Makeflow automatically saves these files in a <tt>makeflow.failed.$ruleno</tt> directory for each failed rule.
Only the specified outputs of a rule will be saved.
If the rule is retried and later succeeds, the failed outputs will be automatically deleted.
</p>

<p>
Finally, Makeflow was created by the Cooperative Computing Lab at the University of Notre Dame.
We are always happy to learn more about how Makeflow is used and assist you if you
have questions or difficulty.  <p>For the latest information about Makeflow, please visit our <a href="http://ccl.cse.nd.edu/software/makeflow">web site</a> and subscribe to our <a href="http://ccl.cse.nd.edu/software">mailing list</a> for more information.
</p>

<a name=batch><h2>Batch Systems</h2></a>

<p>
Makeflow supports a wide variety of batch systems.
Use <tt>makeflow --help</tt> to see the current list supported.
Generally speaking, simply run Makeflow with the <tt>-T</tt> option
to select your desired batch system.  If no option is given,
then <tt>-T local</tt> is assumed.
</p>

<p>
If you need to pass additional parameters to your batch system,
such as specifying a specific queue or machine category,
use the <tt>-B</tt> option to Makeflow, or set the <tt>BATCH_OPTIONS</tt>
variable in your Makeflow file.  The interpretation of these options
is slightly different with each system, as noted below.
</p>

<p>
To avoid overwhelming a batch system with an enormous number of idle jobs,
Makeflow will limit the number of jobs sent to a system at once.
You can control this on the command line with the <tt>--max-remote</tt>
option or the <tt>MAKEFLOW_MAX_REMOTE_JOBS</tt> environment variable.
Likewise, local execution can be limited with <tt>--max-local</tt>
and <tt>MAKEFLOW_MAX_LOCAL_JOBS</tt>.
</p>

<a name=local><h3>Local Execution</h3></a>

By default, Makeflow executes on the local machine.
It will measure the available cores, memory, and disk
on the local machine, and then run as many jobs as fit
in those resources.  (Of course, you must label your jobs
with <tt>CORES</tt>, <tt>MEMORY</tt>, and <tt>DISK</tt> appropriately.
You can put an upper limit on the resources used with the
<tt>--local-cores</tt>, <tt>--local-memory</tt>, and <tt>--local-disk</tt>
options to Makeflow.  Also, the total number of jobs running
locally can be limited with <tt>--max-local</tt>.

<a name=htcondor><h3>HTCondor</h3></a>

<p>Use the <tt>-T condor</tt> option to submit jobs to the <a href=http://research.cs.wisc.edu/htcondor>HTCondor</a> batch system.  (Formerly known as Condor.)</p>

<p>
Makeflow will automatically generate a submit file for each job.
However, if you would like to customize the Condor submit file,
use the <tt>-B</tt> option or <tt>BATCH_OPTIONS</tt> variable
to specify text to add to the submit file.</p>

<p>
For example, if you want to add <tt>Requirements</tt> and <tt>Rank</tt> lines to
your Condor submit files, add this to your Makeflow:
</p>

<code>BATCH_OPTIONS = Requirements = (Memory&gt;1024)</code>

<a name=sge><h3>UGE - Univa Grid Engine / OGE - Open Grid Engine / SGE - Sun Grid Engine</h3></a>

<p>
Use the <tt>-T sge</tt> option to submit jobs to Sun Grid Engine
or systems derived from it like <a href=http://gridscheduler.sourceforge.net>Open Grid Scheduler</a> or <a href=http://www.univa.com/products/>Univa Grid Engine</a>.
</p>

<p>
As above, Makeflow will automatically generate <tt>qsub</tt> commands.
Use the <tt>-B</tt> option or <tt>BATCH_OPTIONS</tt> variable
to specify text to add to the command line.
For example, to specify that jobs should be submitted to the <tt>devel</tt> queue:
</p>

<code>BATCH_OPTIONS = -q devel</code>

<a name=pbs><h3>PBS - Portable Batch System</h3></a>

<p>Use the <tt>-T pbs</tt> option to submit jobs to the <a href=http://www.arc.ox.ac.uk/content/pbs>Portable Batch System</a> or compatible systems.</p>

<p>This will add the values for cores, memory, and disk. These values will be added onto <tt>qsub</tt> in this format:<br>
<tt>-l nodes=1:ppn=${CORES},mem=${MEMORY}mb,file=${DISK}mb</tt></p>

<p>To remove resources specification at submission use Makeflow option <tt>--safe-submit-mode</tt>.</p>

<a name=torque><h3>Torque Batch System</h3></a>

<p>Use the <tt>-T torque</tt> option to submit jobs to the <a href==http://www.adaptivecomputing.com/products/open-source/torque>Torque Resource Manager</a> batch system.</p>

<p>This will add the values for cores, memory, and disk. These values will be added onto <tt>qsub</tt> in this format:<br>
<tt>-l nodes=1:ppn=${CORES},mem=${MEMORY}mb,file=${DISK}mb</tt></p>

<p>To remove resources specification at submission use Makeflow option <tt>--safe-submit-mode</tt>.</p>

<a name=slurm><h3>SLURM - Simple Linux Resource Manager</h3></a>

<p>Use the <tt>-T slurm</tt> option to submit jobs to the <a href=http://slurm.schedmd.com>SLURM</a> batch system.

<p>This will add the values for cores and memory. These values will be added onto <tt>sbatch</tt> in this format:<br>
<tt>-N 1 -n ${CORES} --mem=${MEMORY}M</tt></p>

<p>To remove resources specification at submission use Makeflow option <tt>--safe-submit-mode</tt>.</p>

<p>Disk is not specifies as a shared file system is assumed. The only available disk command line option is
<tt>--tmp</tt>, which governs temporary disk space. This may have unintended limitations on available machines
as temporary disk is highly dependent on host site.</p>

<p>Note: Some SLURM sites disallow specifying memory (example Stampede2). To avoid specification errors
the Makeflow option <tt>--ignore-memory-spec</tt> removes memory from <tt>sbatch</tt>.</p> 

<a name=moab><h3>Moab Scheduler</h3></a>

<p>Use the <tt>-T moab</tt> option to submit jobs to the Moab scheduler or compatible systems.</p>

<a name=mesos><h3>Mesos</h3></a>

<p>
Makeflow can be used with Apache Mesos. To run Makeflow with Mesos,
give the batch mode via <tt>-T mesos</tt> and pass the hostname and 
port number of Mesos master to Makeflow with the <tt>--mesos-master</tt> option.
Since the Makeflow-Mesos Scheduler is based on Mesos Python2 API, the path to Mesos 
Python2 library should be included in the <tt>$PATH</tt>, or one can specify a 
preferred Mesos Python2 API via <tt> --mesos-path </tt> option. To successfully 
import the Python library, you may need to indicate dependencies through 
<tt>--mesos-preload </tt> option. 
</p>

<p>For example, here is the command to run Makeflow on a Mesos cluster
that has the master listening on port 5050 of localhost, with a user 
specified python library:</p>

<code>makeflow -T mesos --mesos-master localhost:5050 
               --mesos-path /path/to/mesos-0.26.0/lib/python2.6/site-packages 
               example.makeflow ...
</code>

<a name=k8s><h3>Kubernetes</h3></a>

<p>
Makeflow can be run on <a href=https://kubernetes.io/>Kubernetes</a> cluster. 
To run Makeflow with Kuberenetes, give the batch mode via <tt>-T k8s</tt> and 
indicate the desired Docker image with the <tt>--k8s-image</tt> option.

The Kubernetes mode is depend on kubectl, before starting Makeflow, make sure kubectl
has been <a href=https://kubernetes.io/docs/tasks/tools/install-kubectl/>installed and correctly set up</a> 
to access the desired Kubernetes cluster. 
</p>

<p>
Following is an example of running Makeflow on a Kuberenetes cluster using
centos image.
</p>

<code>
    makeflow -T k8s --k8s-image "centos" example.makeflow
</code>

<a name=ec2><h3>Amazon Web Services</h3></a>

<p>
Makeflow can be configured to run jobs remotely on Amazon Web Services.
For each job in the workflow, a new virtual machine will be created,
the job will run on the virtual machine, and then the virtual machine
will be destroyed.
</p>

You will need to do some one-time setup first:

<dir>
<li> Create an account with <a href=http://aws.amazon.com>Amazon Web Services</a>, and test it out by creating and deleting some virtual machines manually.
<li> Install the <a href=http://docs.aws.amazon.com/cli/latest/userguide/installing.html>AWS Command Line Interace</a> on the machine where you will
run Makeflow.
<li> Add the <tt>aws</tt> command to your PATH, run <tt>aws configure</tt> and
enter your AWS Access Key ID, your AWS Secret Access Key,
and your preferred region name.  (The keys can be obtained from the AWS
web page by going to your profile menu, selecting "My Security Credentials"
and then "Access Keys".  You may need to create a new access key.)
</dir>

Before proceeding, test that the command-line tool is working by entering:
<code>aws ec2 describe-hosts
</code>

Which should display:
<code>{
    "Hosts": []
}
</code>

Now you are ready to use Makeflow with Amazon.
Before running a workflow, use the <tt>makeflow_ec2_setup</tt> command,
which will setup a virtual private cluster, security group, keypairs, and
other necessary details, and store the necessary information into <tt>my.config</tt>.
Give the name of the default <a href=https://console.aws.amazon.com/ec2/v2/home?#Images>Amazon Machine Image (AMI)</a> you wish to use as the 
second argument:

<code>makeflow_ec2_setup my.config ami-343a694f
</code>

Then, run <tt>makeflow</tt> with the <tt>-T amazon</tt> option
and <tt>--amazon-config my.config</tt> to use the configuration you
just created.  You can run multiple workflows using a single configuration.

<code>makeflow -T amazon --amazon-config my.config example.makeflow ...
</code>

When you are done, destroy the configuration with this command:

<code>makeflow_ec2_cleanup my.config
</code>

<p>
Makeflow selects the virtual machine instance type automatically
by translating the job resources
into an appropriate instance type of the <tt>c4</tt> or <tt>m4</tt> category.
For example, a job requiring
CORES=2 and MEMORY=8000 will be placed into an <tt>m4.large</tt> instance type.
If you do not specify any resources for a job,
then it will be placed into a <tt>t2.micro</tt> instance,
which has very limited performance.  To get good performance,
you should label your jobs with the appropriate <tt>CORES</tt> and <tt>MEMORY</tt>.
</p>

<p>
You can override the choice of virtual machine instance type,
as well as the choice of virtual machine image (AMI) within
the Makeflow on a job-by-job basis by setting the 
<tt>AMAZON_INSTANCE_TYPE</tt> and <tt>AMAZON_AMI</tt>
environment variables within the Makeflow file.  For example:

<code>
export AMAZON_INSTANCE_TYPE=m4.4xlarge
export AMAZON_AMI=ami-343a694f
</code>

<a name=lambda><h3>Amazon Lambda</h3></a>

To use Amazon Lambda, first set up an AWS account and install the <tt>aws</tt> command line tools as noted above.
<p>
Then, use the <tt>makeflow_lambda_setup</tt> command, which will establish
the appropriate roles, buckets, and functions needed to use Amazon Lambda.
The names of these items will be stored in a config file named on the command line:
<pre>
makeflow_lambda_setup my.config
</pre>

Then, you may use <tt>makeflow</tt> normally, indicating <tt>lambda</tt> as
the batch type, and passing in the configuration file with the <tt>--lambda-config</tt> option:

<pre>
makeflow -T lambda --lambda-config my.config example.makeflow ...
</pre>

You may run multiple workflows in this fashion.  When you are ready to
clean up the associated state in Amazon, run <tt>makeflow_lambda_cleanup</tt>
and pass in the configuration file:

<pre>
makeflow_lambda_cleanup my.config
</pre>

<a name=generic><h3>Generic Cluster Submission</h3>

<p>For clusters that are not directly supported by Makeflow we strongly suggest
using the <a href="http://ccl.cse.nd.edu/software/workqueue">Work Queue</a> system
and submitting workers via the cluster's normal submission mechanism.</p>

<p>However, if you have a system similar to Torque, SGE, or PBS which submits
jobs with commands like "qsub", you can inform Makeflow of those commands
and use the <tt>cluster</tt> driver.  For this to work, it is assumed
there is a distributed filesystem shared (like NFS) shared across all
nodes of the cluster.</p>

<p>To configure a custom driver, set the following environment variables:

<ul>
	<li>BATCH_QUEUE_CLUSTER_NAME: The name of the cluster, used in debugging messages and as the name for the wrapper script.</li>
	<li>BATCH_QUEUE_CLUSTER_SUBMIT_COMMAND: The submit command for the cluster (such as qsub or msub)</li>
	<li>BATCH_QUEUE_CLUSTER_SUBMIT_OPTIONS: The command-line arguments that must be passed to the submit command.  This string should end with the argument used to set the name of the task (usually -N).</li>
	<li>BATCH_QUEUE_CLUSTER_REMOVE_COMMAND: The delete command for the cluster (such as qdel or mdel)</li>
</ul>

<p>These will be used to construct a task submission for each makeflow rule that consists of:</p>

<code>$SUBMIT_COMMAND $SUBMIT_OPTIONS $CLUSTER_NAME.wrapper "&lt;rule commandline&gt;"</code>

<p>The wrapper script is a shell script that reads the command to be run as an argument and handles bookkeeping operations necessary for Makeflow.</p>

<a name=workqueue><h2>Using Work Queue</h2></a>

<a name=wqoverview><h3>Overview</h3></a>

<p>As described so far, Makeflow translates each job in the workflow
into a single submission to the target batch system.  This works well
as long as each job is relatively long running and doesn't perform
a large amount of I/O.
However, if each job is relatively short, the workflow may run very
slowly, because it can take 30 seconds or more for the batch system to
start an individual job.  If common files are needed by each job,
they may end up being transferred to the same node multiple times.
</p>

<p>To get around these limitations, we provide the
<a href=http://ccl.cse.nd.edu/software/workqueue</a>Work Queue</a> system.
The basic idea is to submit a number of persistent "worker" processes
to an existing batch system.  Makeflow communicates directly with the
workers to quickly dispatch jobs and cache data, rather than interacting
with the underlying batch system.  This accelerates short jobs and
exploits common data dependencies between jobs.
</p>

<p>To begin, let's assume that you are logged into the head node
of your cluster, called <tt>head.cluster.edu</tt>
Run Makeflow in Work Queue mode like this:
</p>

<code>makeflow -T wq example.makeflow</code>

<p>Then, submit 10 worker processes to Condor like this:</p>

<code>condor_submit_workers head.cluster.edu 9123 10
Submitting job(s)..........
Logging submit event(s)..........
10 job(s) submitted to cluster 298.
</code>

<p>Or, submit 10 worker processes to SGE like this:</p>

<code>sge_submit_workers head.cluster.edu 9123 10</code>

<p>Or, you can start workers manually on any other machine you can log into:</p>

<code>work_queue_worker head.cluster.edu 9123</code>

<p>Once the workers begin running, Makeflow will dispatch multiple tasks to
each one very quickly.  If a worker should fail, Makeflow will retry the work
elsewhere, so it is safe to submit many workers to an unreliable system.</p>

<p>When the Makeflow completes, your workers will still be available, so you
can either run another Makeflow with the same workers, remove them from the
batch system, or wait for them to expire.  If you do nothing for 15 minutes,
they will automatically exit.</p>

<p>Note that <tt>condor_submit_workers</tt> and <tt>sge_submit_workers</tt>,
are simple shell scripts, so you can edit them directly if you would like to
change batch options or other details. Please refer to <a href="workqueue.html"</a> Work Queue manual </a> for more details.

<a name=ports><h3>Port Numbers</h3></a>

<p>Makeflow listens on a port which the remote workers would connect to.  The
default port number is 9123.  Sometimes, however, the port number might be not
available on your system. You can change the default port via the <tt>-p</tt> option. For example, if you want the master to listen
on port 9567 by default, you can run the following command:</p>

<code>makeflow -T wq -p 9567 example.makeflow</code>

<a name=names><h3>Project Names</h3></a>

<p>If you don't like using port numbers, an easier way to match workers to masters is to use a project name.  You can give each master a project name with the -N option.</p>

<code>makeflow -T wq -N MyProject example.makeflow </code>

<p>The -N option gives the master a project name called 'MyProject',
and will cause it to advertise its information
such as the project name, running status, hostname and port number, to a <A href=catalog.html>catalog server</a>.
Then a worker can simply identify the workload by its project name.
</p>

<p>To start a worker that automatically finds MyProject's master via the default
catalog server:</p>

<code>work_queue_worker -N MyProject</code>

<p>You can also give multiple -N options to a worker. The worker will find out
which ones of the specified projects are running from the catalog server and
randomly select one to work for. When one project is done, the worker would
repeat this process.  Thus, the worker can work for a different master without
being stopped and given the different master's hostname and port. An example of
specifying multiple projects:</p>

<code>work_queue_worker -N proj1 -N proj2 -N proj3</code>

<p>
In addition to creating a project name using the -N option, this will also trigger automatic reporting to the 
designated catalog server. The Port and Server address are taken from the environment variables <b>CATALOG_HOST</b> 
and <b>CATALOG_PORT</b>. If either variable is not set, then the addresses "catalog.cse.nd.edu,backup-catalog.cse.nd.edu" and port 9097 will be used.
</p>
<p>
It is also easy to run your own catalog server, if you prefer.
For more details, see the <a href=catalog.html>Catalog Server Manual</a>.
</p>

<a name=password><h3>Setting a Password</h3></a>

<p>We recommend that anyone using the catalog server also set a password.
To do this, select any password and write it to a file called
<tt>mypwfile</tt>.  Then, run Makeflow and each worker with the
<tt>--password</tt> option to indicate the password file:</p>

<code>makeflow <b>--password</b> mypwfile ...
work_queue_worker <b>--password</b> mypwfile ...
</code>

<a name=container><h2>Container Environments</h2></a>

<p>
Makeflow can interoperate with a variety of container technologies,
including Docker, Singularity, and Umbrella.  In each case, you name
the container image on the command line, and then Makeflow will apply
it to each job by creating the container, loading (or linking)
the input files into the container, running the job, extracting
the output files, and deleting the container.
</p>

<p>
Note that container specification is independent of batch system selection.
For example, if you select HTCondor execution with <tt>-T condor</tt> and
Docker support with <tt>--docker</tt>, then Makeflow will submit HTCondor
batch jobs, each consisting of an invocation of <tt>docker</tt> to run the job.
You can switch to any combination of batch system and container technology that you like.
</p>

<a name=docker><h3>Docker</h3></a>

<p>
If you have the Docker container enviornment installed on your cluster,
Makeflow can be set up to place each job in your workflow into a Docker container.
Invoke Makeflow with the <tt>--docker</tt> argument followed by the container image to use.
Makeflow will ensure that the named image is pulled into each Docker node, and then execute the task within that container.  For example, <tt>--docker debian</tt>
will cause all tasks to be run in the container name <tt>debian</tt>.
</p>

<p>Alternatively, if you have an exported container image, you can use the exported image via the <tt>--docker-tar</tt> option.
Makeflow will load the container into each execution node as needed.  This allows you to use a container without pushing it to a remote repository.
</p>

<a name=singularity><h3>Singularity</h3></a>

<p>
In a similar way, Makeflow can be used with the Singularity container system.
When using this mode, Singularity will take in an image, set up the container, and runs the command inside of the container.
Any needed input files will be read in from Makeflow, and created files will be delivered by Makeflow.
</p>

<p> Invoke Makeflow with the <tt>--singularity</tt> argument, followed by the path to the desired image file.
Makeflow will ensure that the named image will be transferred to each job, using the appropriate mechanism
for that batch system
</p>

<a name=umbrella><h3>Umbrella</h3></a>

<p>Makeflow allows the user to specify the execution environment for each rule
via its <tt>--umbrella-spec</tt> and <tt>--umbrella-binary</tt> options.  The
<tt>--umbrella-spec</tt> option allows the user to specify an Umbrella
specification, the <tt>--umbrella-binary</tt> option allows the user to specify
the path to an Umbrella binary.  Using this mode, each rule will be converted
into an Umbrella job, and the specified Umbrella specification and binary will
be added into the input file list of a job.  </p>

To test makeflow with umbrella using local execution engine:
<code>makeflow --umbrella-binary $(which umbrella) --umbrella-spec convert_S.umbrella example.makeflow
</code>

To test makeflow with umbrella using wq execution engine:
<code>makeflow -T wq --umbrella-binary $(which umbrella) --umbrella-spec convert_S.umbrella example.makeflow
</code>

<p> To run each makeflow rule as an Umbrella job, the <tt>--umbrella-spec</tt>
must be specified.  However, the <tt>--umbrella-binary</tt> option is optional:
when it is specified, the specified umbrella binary will be sent to the
execution node; when it is not specified, the execution node is expected to
have an umbrella binary available through the <tt>$PATH</tt> environment
variable. </p>

<p> Makeflow also allows the user to specify the umbrella log file prefix via
its <tt>--umbrella-log-prefix</tt> option. The umbrella log file is in the
format of "<umbrella-log-prefix>.<rule_id>". The default value for the
<tt>--umbrella-log-prefix</tt> option is "<makefile_path>.umbrella.log". </p>

<p> Makeflow also allows the user to specify the umbrella execution mode via
its <tt>--umbrella-mode</tt> option. Currently, this option can be set to the
following three modes: local, parrot, and docker. The default value of the
<tt>--umbrella-mode</tt> option is local, which first tries to utilize the
docker mode, and tries to utilize the parrot mode if the docker mode is not
available. </p>

<p> You can also specify an Umbrella specification for a group of rule(s) in the
Makefile by putting the following directives before the rule(s) you want to apply
the Umbrella spec to: </p>
<code>
.MAKEFLOW CATEGORY 1
.UMBRELLA SPEC convert_S.umbrella
</code>

<p>In this case, the specified Umbrella spec will be applied to all the
following rules until a new ".MAKEFLOW CATEGORY ..." directive is declared. All
the rules before the first ".MAKEFLOW CATEGORY ..." directive will use the
Umbrella spec specified by the <tt>--umbrella-spec</tt> option. If the
<tt>--umbrella-spec</tt> option is not specified, these rules will run without being
wrapped by Umbrella. </p>

<a name=wrapper><h3>Wrapper Commands</h3></a>

Makeflow allows a global wrapper command to be applied to every rule in the workflow.  This is particularly useful for applying troubleshooting tools, or for setting up a global environment without rewriting the entire workflow.  The <tt>--wrapper</tt> option will prefix a command in front of every rule, while the <tt>--wrapper-input</tt> and <tt>--wrapper-output</tt> options will specify input and output files related to the wrapper.</p>

<p>A few special characters are supported by wrappers.  If the wrapper command or wrapper files contain two percents (<tt>%%</tt>), then the number of the current rule will be substituted there.  If the command contains curly braces (<tt>{}</tt>) the original command will be substituted at that point.
Square brackets (<tt>[]</tt>) are the same as curly braces, except that the command is quoted and escaped before substitution.
If neither specifier is given, Makeflow appends <tt>/bin/sh -c []</tt> to the wrapper command.</p>

<p>For example, suppose that you wish to shell builtin command <tt>time</tt> to
every rule in the workflow.  Instead of modifying the workflow, run it like
this:</p>

<code>makeflow --wrapper 'time -p' example.makeflow</code>

<p>Since the preceding wrapper did not specify where to substitute the command, it is equivalent to</p>
<code>makeflow --wrapper 'time -p /bin/sh -c []' example.makeflow</code>
<p>This way, if a single rule specifies multiple commands, the wrapper will time <em>all</em> of them.</p>

<p>The square brackets and the default
behavior of running commands in a shell were added because Makeflow allows a rule to run multiple commands. The curly braces simply perform text substitution, so for example</p>
<code>makeflow --wrapper 'env -i {}' example.makeflow</code>
does not work correctly if multiple commands are specified.
<code>target_1: source_1
	command_1; command_2; command_3</code>
will be executed as
<code>env -i command_1; command_2; command_3</code>
<p>Notice that only <tt>command_1</tt>'s environment will be cleared; subsequent commands are not affected.
Thus this wrapper should be given as</p>
<code>makeflow --wrapper 'env -i /bin/sh -c []' example.makeflow</code>
or more succinctly as
<code>makeflow --wrapper 'env -i' example.makeflow</code>

<p>Suppose you want to apply <tt>strace</tt> to every rule, to obtain system call traces.  Since every rule would have to have its own output file for the trace, you could indicate output files like this:</p>

<code>makeflow --wrapper 'strace -o trace.%%' --wrapper-output 'trace.%%' example.makeflow</code>

<p>Suppose you want to wrap every command with a script that would set up an appropriate Java environment.  You might write a script called <tt>setupjava.sh</tt> like this:</p>

<code>#!/bin/sh
export JAVA_HOME=/opt/java-9.8.3.6.7
export PATH=${JAVA_HOME}/bin:$PATH
echo "using java in $JAVA_HOME"
exec "$@"
</code>

<p>And then invoke Makeflow like this:</p>

<code>makeflow --wrapper ./setupjava.sh --wrapper-input setupjava.sh example.makeflow</code>

<a name=mpi><h2>Running in an MPI Environment</h2></a>

If you have access to an MPI cluster, Makeflow and CCTools can also take advantage of these systems. First, CCTools needs to be configured and built with <tt>mpicc</tt>. Once done, there are two methods
for employing Makeflow and CCTools with MPI: directly running Makeflow as and MPI Job, and Makeflow+WorkQueue as an MPI Job.

<a name=makeflow-mpi><h3>Running Makeflow as an MPI Job</h3></a>
If cctools and Makeflow has been configured and built with <tt>mpicc</tt>, then Makeflow can be ran as an MPI program by passing Makeflow and all of its arguments as arguments to <tt>mpirun</tt> or <tt>mpiexec</tt>. An example of using it as such looks like 
<code>mpirun -np 120 makeflow -T mpi example.makeflow</code> 
As the example shows, <tt>-T mpi</tt> is necessary to inform Makeflow that is is being ran as an mpi program. When doing so, Makeflow will count how many processes report back to the master/RANK0 process on each node, and use that as how many cores exist on each node. A single process on each node is then ran as the worker process, with RANK0 acting as the master process, dispatching jobs and acting as Makeflow usually does. Each worker will then share its memory fairly amongst all logical cores and the number of cores Makeflow measures that it gets to use on each node. This is done by following the equation <tt> worker_memory = (total_memory / total_logical_cores) * cores_for_worker</tt>. To overrite these options, use the commands <tt>--mpi-cores=NUM --mpi-memory=NUM</tt> to set the number of cores each worker should use, and how much memory each worker gets to use.
Calling <tt>mpirun makeflow -T mpi</tt> can also be done in your batch system's submission script, or other methods of submitting an MPI job.

<a name=mpi-starter><h3>Running Makeflow and WorkQueue as an MPI Job</h3></a>
An alternative way of using Makeflow with MPI is to couple it with WorkQueue and submit both as an MPI job. You can do this by using <tt>makeflow_mpi_starter</tt>. This allows the user to submit a single program which will start Makeflow on the Rank0 process, and WorkQueue workers on all of the nodes. It behaves the same way as using Makeflow as an MPI program does in partitioning cores and memory on the workers. using it is very simple, as seen here:

<code>
mpiexec -np 120 makeflow_mpi_starter -m "-r 10 example.makeflow" -q "" -p 9000
</code>

In the above code, we turn on retries for failed jobs for Makeflow, setting the maximum number of retries to 10. We don't pass anything to WorkQueue, and we set the port for Makeflow to 9000. This method can be a helpful way to take advantage of both WorkQueue while using an MPI partition, when using Makeflow. Because we are passing the arguments of <tt>-m</tt> and <tt>-q</tt> directly to Makeflow and WorkQueue respectively, you can overwrite the core and memory partitioning by using the normal commands for Makeflow and WorkQueue (e.g. <tt>--local-cores=1</tt> and <tt>--cores=10</tt>).

Just like the previous use case, this can also be used as the command in your batch system's submission script, or other method of submitting an MPI job.
<a name=advanced><h2>Advanced Features</h2></a>

<a name=shared><h3>Shared File Systems</h3></a>

<p>By default, Makeflow does <i>not</i> assume that your cluster has a shared
filesystem like NFS or HDFS, and so, to be safe, it copies all necessary dependencies for each job.  However, if you do have a shared filesystem, it can
be used to efficiently access files without making remote copies.
Makeflow must be told where the shared filesystem is located, so that it
can take advantage of it.  To enable this, invoke Makeflow with the 
<tt>--shared-fs</tt> option, indicating the path where the shared
filesystem is mounted.  (This option can be given multiple times.)
Makeflow will then verify the existence of input and output files in these
locations, but will not cause them to be transferred.
</p>

<p>
For example, if you use NFS to access the <tt>/home</tt> and
<tt>/software</tt> directories on your cluster, then invoke makeflow like this:
</p>

<code>makeflow --shared-fs /home --shared-fs /software example.makeflow ...
</code>

<a name=consistency><h3>NFS Consistency Delay</h3></a>

<p>
After a job completes, Makeflow checks that the expected output files
were actually created.  However, if the output files were written via
NFS (or another shared filesystem), it is possible that the outputs
may not be visible for a few seconds.  This is due to caching and buffering
effects in many filesystems.
</p>

<p>
If you experience this problem,  you can instruct Makeflow to retry
the output file check for a limited amount of time, before concluding
that the files are not there. Use the
<tt>--wait-for-files-upto</tt> option to specify the number of seconds
to wait.
</p>

<a name=mounting><h3>Mounting Remote Files</h3></a>

<p>
It is often convenient to separate the <i>logical</i> purpose of
a file from it's <i>physical location</i>.  For example, you may
have a workflow which makes use of a large reference database called
<tt>ref.db</tt> which is a standard file downloaded from a common data
repository whenever needed.
</p>
<p>
Makeflow allows you to map logical file names within the workflow
to arbitrary locations on disk, or downloaded from URLs.
This is accomplished by writing a "mount file" in which each line
lists a logical file name and its physical location.
</p>

<p>Here is an example of a mountfile:</p>

<code>curl /usr/bin/curl
convert ../bin/convert
data/file1 /home/bob/input1
1.root http://myresearch.org/1.root
</code>

<p>Then, simply execute Makeflow with the <tt>--mounts</tt> option
to apply the desred mountfile:</p>

<code>makeflow --mounts my.mountfile example.makeflow ...
</code>

<p>Before execution, Makeflow first parses each line of the mountfile when the <tt>--mounts</tt> option is set, and copies the specified dependency from the location specified by <tt>source</tt> field into a local cache, and then links the <tt>target</tt> to the item under the cache. Makeflow also records the location of the local cache and the info (source, target, filename under the cache dir, and so on) of each dependencies specified in the mountfile into its log.
</p>

<p>To cleanup a makeflow together with the local cache and all the links created due to the mountfile:</p>

<code>makeflow -c example.makeflow</code>

<p>By default, makeflow creates a unique directory under the current working
directory to hold all the dependencies introduced by the mountfile.
This location can be adjusted with the <tt>--cache-dir</tt> option.

<p>To only cleanup the local cache and all the links created due to the mountfile:</p>

<code>makeflow -ccache example.makeflow</code>

<p>To limit the behavoir of a makeflow inside the current working directory, the <tt>target</tt> field should satisfy the following requirements:</p>
<ul>
  <li>the target path should not exist;</li>
  <li>the target path should be relative;</li>
  <li>the target path should not include any symbolic links; (For example,
  if the target path is <tt>a/b/c/d</tt>, then <tt>a</tt>, <tt>b</tt>,
  <tt>c</tt> should not be symbolic links.) This avoids the makeflow
  leaving the current working directory through symbolic links.</li>
  <li>the target path should not include doubledots (<tt>..</tt>) to limit
  makeflow to the current working directory.  For example, <tt>a/../b</tt>
  is an invalid target, however, <tt>a/b..c/d</tt> is a valid target. </li>
  <li>the target path should be the same with its usage in Makeflow makefiles.
  For example, the target path for the path <tt>./mycurl</tt> from a makefile
  must also be <tt>./mycurl</tt>, can not be <tt>mycurl</tt>.
</ul>

<p>The <tt>source</tt> field can be a local file path or a http URL. When a local file path is specified,
the following requirements should be satisfied:</p>
<ul>
  <li>the source path should exist;</li>
  <li>the source path can be relative or absolute;</li>
  <li>the source path can include doubledots (<tt>..</tt>);</li>
  <li>the source path should only be regular files, directories, and symbolic links;</li>
  <li>the source path should not be any ancestor directory of the target to avoid infinite loop;</li>
</ul>

<p>To limit the behavoir of a makeflow inside the current working directory, the <tt>cache_dir</tt> should satisfy the following requirements:</p>
<ul>
  <li>the cache_dir path should be a relative path;</li>
  <li>the cache_dir path should not include any symbolic link; (For example,
  if the cache_dir path is <tt>a/b/c/d</tt>, then <tt>a</tt>, <tt>b</tt>,
  <tt>c</tt> should not be symbolic link.) This limitation aims to avoid the makeflow
  leaving the current working directory through symbolic links.</li>
  <li>the cache_dir path should not include doubledots (<tt>..</tt>) to limit the
  behavoir of the makeflow within the current working directory.  For example, <tt>a/../b</tt>
  is an invalid cache_dir, however, <tt>a/b..c/d</tt> is a valid cache_dir. </li> </ul>
</ul>

<a name=garbage><h3>Garbage Collection</h3></a>

<p>As the workflow execution progresses, Makeflow can automatically delete
intermediate files that are no longer needed. In this context, an intermediate
file is an input of some rule that is the target of another rule. Therefore, by
default, garbage collection does not delete the original input files, nor <b>
final</b> target files.</p>

<p>Which files are deleted can be tailored from the default by appending files
to the Makeflow variables <tt>MAKEFLOW_INPUTS</tt> and <tt>MAKEFLOW_OUTPUTS</tt>.
Files added to <tt>MAKEFLOW_INPUTS</tt> augment the original inputs files that
should not be deleted. <tt>MAKEFLOW_OUTPUTS</tt> marks final target files that
should not be deleted.  However, different from <tt>MAKEFLOW_INPUTS</tt>, files
specified in <tt>MAKEFLOW_OUTPUTS</tt> does not include all output files. If
<tt>MAKEFLOW_OUTPUTS</tt> is not specified, then all files not used in subsequent
rules are considered outputs. It is considered best practice to always specify
<tt>MAKEFLOW_INPUTS/OUTPUTS</tt> to clearly specify which files are considered
inputs and outputs and allow for better space management if garbage collection
is used.</p>

<p>Makeflow offers two modes for garbage collection: reference count, and on
demand. With the reference count mode, intermediate files are deleted as soon
as no rule has them listed as input. The on-demand mode is similar to reference
count, only that files are deleted until the space on the local file system is
below a given threshold.</p>

<p>To activate reference count garbage collection:</p>

<code>makeflow -gref_cnt</code>

<p>To activate on-demand garbage collection, with a threshold of 500MB:</p>

<code>makeflow -gon_demand -G500000000</code>

<a name=viz><h3>Visualization</h3></a>

<p>There are several ways to visualize both the structure of a Makeflow
as well as its progress over time.
<tt>makeflow_viz</tt> can be used to convert a Makeflow into
a file that can be displayed by <a href="http://www.graphviz.org">Graphviz DOT</a> tools like this:</p>

<code>makeflow_viz -D dot example.makeflow &gt; example.dot
dot -Tgif &lt; example.dot &gt; example.gif
</code>
<p>
Or use a similar command to generate a <a href="http://www.cytoscape.org">Cytoscape</a> input file.  (This will also create a Cytoscape <tt>style.xml</tt> file.)</p>

<code>makeflow_viz -D cyto example.makeflow > example.xgmml</code>

<p>To observe how a makeflow runs over time, use <tt>makeflow_graph_log</tt> to convert a log file into a timeline that shows the number of tasks ready, running, and complete over time:</p>

<code>makeflow_graph_log example.makeflowlog example.png</code>

<a name=archiving><h3>Archiving Jobs</h3></a>

<p><tt>Makeflow</tt> allows for users to archive the results of each job within a specified archive directory. This is done using the <tt>--archive</tt> option, which by default creates an archiving directory at <tt>/tmp/makeflow.archive.$UID</tt>. Both files and jobs are stored as the workflow executes. <tt>Makeflow</tt> will also check to see if a job has already been archived into the archiving directory, and if so the outputs of the job will be copied to the working directory and the job will skip execution.</p>

<code> makeflow --archive example.makeflow </code>

<p> To only write to the archiving directory (and ensure that all nodes will be executed instead), pass <tt>--archive-write</tt>. To only read from the archive and use the outputs of any archived job, pass <tt>--archive-read</tt>. To specify a directory to use for the archiving directory, give an optional argument as shown below </p>

<code> makeflow --archive=/path/to/directory/ example.makeflow</code>

<p> The archive also has an option to upload and download workflow contents from and Amazon Web Services S3 bucket. This is done using the <tt>--archive-s3</tt> option, which by default uploads/downloads from the S3 bucket name <tt>makeflows3archive</tt>. <tt>Makeflow</tt> will also check to see if files have already been uploaded to the S3 bucket so files are not uploaded twice, wasting time. Archiving with Amazon S3 utilizes the same functionality that the regular archive tool uses locally with copying files to and from <tt>/tmp/makeflow.archive.$UID</tt>. The user will need to make sure that their aws key id and secret key are stored within the credentials file that is provided when you configure AWS. </p>

<code> makeflow --archive-s3= Amazon S3 Bucket Name</code> 

<p> If you do not want to check to see if files exist when uploading you can use the other option <tt>--archive-s3-no-check</tt>, which has the same default S3 bucket and options to change the bucket name.</p>

<code> makeflow --archive-s3-no-check= Amazon S3 Bucket Name</code>

<a name=linking><h3>Linking Dependencies</h3></a>

<p><tt>Makeflow</tt> provides a tool to collect all of the dependencies for a
given workflow into one directory. By collecting all of the input files and
programs contained in a workflow it is possible to run the workflow on other
machines.</p>

<p>Currently, <tt>Makeflow</tt> copies all of the files specified as
dependencies by the rules in the makeflow file, including scripts and data
files. Some of the files not collected are dynamically linked libraries,
executables not listed as dependencies (<tt>python</tt>, <tt>perl</tt>), and
configuration files (<tt>mail.rc</tt>).</p>

<p>To avoid naming conflicts, files which would otherwise have an identical
path are renamed when copied into the bundle:</p>

<ul>
  <li>All file paths are relative to the top level directory.</li>
  <li>The makeflow file is rewritten with the new file locations and placed in the top level directory.</li>
  <li>Files originally specified with an absolute path are placed into the top level directory.</li>
  <li>Files with the same path are appended with a unique number</li>
</ul>

<p>Example usage:</p>

<code>makeflow_analyze -b some_output_directory example.makeflow</code>

<a name=reference><h2>Technical Reference</h2></a>

<a name=language><h3>Language Reference</h3></a>

<p>The Makeflow language is very similar to Make, but it does have a few important differences that you should be aware of.</p>

<h4>Get the Dependencies Right</h4>

<p>You must be careful to accurately specify <b>all of the files that a rule
requires and creates</b>, including any custom executables.  This is because
Makeflow requires all these information to construct the environment for a
remote job.  For example, suppose that you have written a simulation program
called <tt>mysim.exe</tt> that reads <tt>calib.data</tt> and then produces and
output file.  The following rule won't work, because it doesn't inform Makeflow
what files are neded to execute the simulation:</p>

<code># This is an incorrect rule.

output.txt:
        ./mysim.exe -c calib.data -o output.txt
</code>

<p>However, the following is correct, because the rule states all of the files
needed to run the simulation.  Makeflow will use this information to construct
a batch job that consists of <tt>mysim.exe</tt> and <tt>calib.data</tt> and
uses it to produce <tt>output.txt</tt>:</p>

<code># This is a correct rule.

output.txt: mysim.exe calib.data
        ./mysim.exe -c calib.data -o output.txt
</code>

<p>Note that when a directory is specified as an input dependency, it
means that the command relies on the directory and all of its contents.
So, if you have a large collection of input data, you can place it
in a single directory, and then simply give the name of that directory.
</p>

<h4>No Phony Rules</h4>

<p>For a similar reason, you cannot have "phony" rules that don't actually
create the specified files.  For example, it is common practice to define a
<tt>clean</tt> rule in Make that deletes all derived files.  This doesn't make
sense in Makeflow, because such a rule does not actually create a file named
<tt>clean</tt>.  Instead use the <tt>-c</tt> option as shown above.</p>

<h4>Just Plain Rules</h4>

<p>Makeflow does not support all of the syntax that you find in various
versions of Make.  Each rule must have exactly one command to execute.  If you
have multiple commands, simply join them together with semicolons.  Makeflow
allows you to define and use variables, but it does not support  pattern rules,
wildcards, or special variables like <tt>$&lt;</tt> or <tt>$@</tt>.  You simply
have to write out the rules longhand, or write a script in your favorite
language to generate a large Makeflow.</p>

<h4>Local Job Execution</h4>

<p>Certain jobs don't make much sense to distribute.  For example, if you have
a very fast running job that consumes a large amount of data, then it should
simply run on the same machine as Makeflow.  To force this, simply add the word
<tt>LOCAL</tt> to the beginning of the command line in the rule.</p>

<h4>Rule Lexical Scope</h4>

<p>Variables in Makeflow have global scope, that is, once defined, their value
can be accessed from any rule. Sometimes it is useful to define a variable
locally inside a rule, without affecting the global value. In Makeflow, this
can be achieved by defining the variables after the rule's requirements, but
before the rule's command, and prepending the name of the variable with
<tt>@</tt>, as follows:</p>

<code>SOME_VARIABLE=original_value

target_1: source_1
	command_1

target_2: source_2
@SOME_VARIABLE=local_value_for_2
	command_2

target_3: source_3
	command_3 </code>

In this example, SOME_VARIABLE has the value 'original_value' for rules 1 and
3, and the value 'local_value_for_2' for rule 2.

<h4>Environment Variables</h4>

Environment variables can be defined with the <tt>export</tt> keyword inside a workflow.  Makeflow will communicate explicitly named environment variables
to remote batch systems, where they will override whatever local setting is present.  For example, suppose you want to modify the PATH for every job in the makeflow:

<code>export PATH=/opt/tools/bin:${PATH}</code>

If no value is given, then the current value of the environment variable is passed along to the job:

<code>export USER</code>

<h4 id="details.renaming">Remote File Renaming<a class="sectionlink" href="#details.renaming" title="Link to this section.">&#x21d7;</a></h4>

<p>When the underlying batch system supports it,
Makeflow supports "Remote Renaming" where the name
of a file in the execution sandbox is different than
the name of the same file in the DAG.
This is currently supported by the Work Queue, Condor, and Amazon batch system.
</p>

<p>
For example, <tt>local_name-&gt;remote_name</tt> indicates that the file <tt>local_name</tt> is called <tt>remote_name</tt>
in the remote system. Consider the following example:</p>

<code>b.out: a.in myprog
LOCAL myprog a.in &gt; b.out

c.out-&gt;out: a.in-&gt;in1 b.out myprog-&gt;prog
        prog in1 b.out &gt; out
</code>

<p>The first rule runs locally, using the executable <tt>myprog</tt> and the
local file <tt>a.in</tt> to locally create <tt>b.out</tt>.  The second rule
runs remotely, but the remote system expects <tt>a.in</tt> to be named
<tt>in1</tt>, <tt>c.out</tt>, to be named <tt>out</tt> and so on. Note that we
did not need to rename the file <tt>b.out</tt>. Without remote file renaming,
we would have to create either a symbolic link, or a copy of the files with the
expected correct names.</p>

<h4>Nested Makeflows</h4>

One Makeflow can be nested inside of another by writing a rule
with the following syntax:

<code>output-files: input-files
    MAKEFLOW makeflow-file [working-dir]
</code>

The input and output files are specified as usual, describing the files
consumed and created by the child makeflow as a whole.  Then, the <tt>MAKEFLOW</tt> keyword introduces the child makeflow specification, and an
optional working directory into which the makeflow will be executed.
If not given, the current working directory is assumed.

<a name=rescat><h3>Resources and Categories</h3></a>

<p>
Makeflow has the capability of automatically setting the cores, memory,
and disk space requirements to the underlying batch system (currently this only
works with Work Queue and Condor). Jobs are grouped into <em> job categories
</em>, and jobs in the same category have the same cores, memory, and disk
requirements.
</p>
<p>
Job categories and resources are specified with variables.  Jobs are assigned
to the category named in the value of the variable CATEGORY.  Likewise, the
values of the variables CORES, MEMORY (in MB), and DISK (in MB) describe the
resources requirements for the category specified in CATEGORY.
</p>
<p>
Jobs without an explicit category are assigned to <tt>default</tt>.  Jobs in
the default category get their resource requirements from the value of the
environment variables CORES, MEMORY, and DISK.
</p>

Consider the following example:

<code> # These tasks are assigned to the category preprocessing.
# MEMORY and CORES are read from the environment, if defined.

CATEGORY="preprocessing"
DISK=500

one: src
    cmd

two: src
    cmd

# These tasks have the category "simulation". Note that now CORES, MEMORY, and DISK are specified.

CATEGORY="simulation"
CORES=1
MEMORY=400
DISK=400

three: src
    cmd

four: src
    cmd

# Another category switch. MEMORY is read from the environment.

CATEGORY="analysis"
CORES=4
DISK=600

five: src
    cmd
</code>

<code>export MEMORY=800
makeflow ...
</code>

<h4>Resources Specified</h4>

<table>
<tr><th>Category</th><th>Cores</th><th>Memory (MB)</th><th>Disk (MB)</th></tr>
<tr><td> preprocessing </td> <td> (unspecified) </td> <td> 800 (from environment) </td> <td> 500 </td> </tr>
<tr><td> simulation </td> <td> 1 </td> <td> 400 </td> <td> 400 </td> </tr>
<tr><td> analysis </td> <td> 4 </td> <td> 800  (from environment)</td> <td> 600 </td> </tr>
</table>

<a name=log><h3>Transaction Log</h3>

<p>
As Makeflow runs, it creates a <i>transaction log</i> that records the details
of each job, where it was sent, how long it ran, and what resources it consumed.
By default, this log is named <tt>X.makeflowlog</tt> where X is the name of the
original makeflow file.
</p>

<p>
The transaction log serves several purposes:
<ol>
<li><b>Recovery.</b>  The transaction log allows Makeflow to continue where it left off.
If you must restart Makeflow after a crash, it will read the transaction log, determine
what jobs are complete (or still running) and then continue forward from there.
<li><b>Cleanup.</b>  The <tt>--clean</tt> option relies on the transaction log to quickly
determine exactly which files have been created and which jobs have been submitted,
so that they can be quickly and precisely deleted and removed.  (There is no need to
create a <tt>clean</tt> rule by hand, as you would in traditional Make.)
<li><b>Monitoring.</b>  Tools like <tt>makeflow_monitor</tt> and <tt>makeflow_graph_log</tt>
read the transaction log to determine the current state of the workflow and display it
to the user.
</ol>

<p>A sample transaction log might look like this:</p>

<code># STARTED	1435251570723463
# 1 capitol.jpg 1435251570725086
1435251570725528 5 1 17377 5 1 0 0 0 6
# 2 capitol.jpg 1435251570876426
1435251570876486 5 2 17377 5 0 1 0 0 6
# 1 capitol.360.jpg 1435251570876521
1435251570876866 4 1 17379 4 1 1 0 0 6
# 1 capitol.270.jpg 1435251570876918
1435251570877166 3 1 17380 3 2 1 0 0 6
# 2 capitol.270.jpg 1435251570984114
1435251570984161 3 2 17380 3 1 2 0 0 6
# 1 capitol.180.jpg 1435251570984199
1435251570984533 2 1 17383 2 2 2 0 0 6
# 2 capitol.360.jpg 1435251571003847
1435251571003923 4 2 17379 2 1 3 0 0 6
# 1 capitol.90.jpg 1435251571003969
1435251571004476 1 1 17384 1 2 3 0 0 6
# 2 capitol.180.jpg 1435251571058319
1435251571058369 2 2 17383 1 1 4 0 0 6
# 2 capitol.90.jpg 1435251571094157
1435251571094214 1 2 17384 1 0 5 0 0 6
# 1 capitol.anim.gif 1435251571094257
1435251571094590 0 1 17387 0 1 5 0 0 6
# 2 capitol.anim.gif 1435251575980215
# 3 capitol.360.jpg 1435251575980270
# 3 capitol.270.jpg 1435251575980288
# 3 capitol.180.jpg 1435251575980303
# 3 capitol.90.jpg 1435251575980319
# 3 capitol.jpg 1435251575980334
1435251575980350 0 2 17387 0 0 6 0 0 6
# COMPLETED	1435251575980391
</code>

<p>
Each line in the log file represents a single action taken on a single rule in the workflow.
For simplicity, rules are numbered from the beginning of the Makeflow, starting with zero.
Each line contains the following items:
</p>
<code>timestamp task_id new_state job_id tasks_waiting tasks_running tasks_complete tasks_failed tasks_aborted task_id_counter</code>

<p>Which are defined as follows:</p>

<ul>
	<li><b>timestamp</b> - the unix time (in microseconds) when this line is written to the log file. </li>
	<li><b>task_id</b> - the id of this n. </li>
	<li><b>new_state</b> - a integer represents the new state this task (whose id is in the task_id column) has just entered. The value of the integer ranges from 0 to 4 and the states they are representing are:</li>
	<ol start="0">
		<li> waiting </li>
		<li> running </li>
		<li> complete </li>
		<li> failed </li>
		<li> aborted </li>
	</ol>
	<li><b>job_id</b> - the underline execution system is a batch system, such as Condor or SGE,
	the job id would be the job id assigned by the batch system when the task
	was sent to the batch system for execution.</li>
	<li><b>tasks_waiting</b> - the number of tasks are waiting to be executed.</li>
	<li><b>tasks_running</b> - the number of tasks are being executed.</li>
	<li><b>tasks_complete</b> - the number of tasks has been completed.</li>
	<li><b>tasks_failed</b> - the number of tasks has failed.</li>
	<li><b>tasks_aborted</b> - the number of tasks has been aborted.</li>
	<li><b>task_id_counter</b> - total number of tasks in this Makeflow</li>
</ul>

<p> In addition, lines starting with a pound sign are comments and
contain additional high-level information that can be safely ignored.
The transaction log begins with a comment to indicate the starting time, and ends with
a comment indicating whether the entire workflow completed, failed, or was aborted.
</p>

<p>Aside from the high-level information, file states are also recorded in the log.
This allows for tracking files throughout the workflow execution. This information
is shown starting with the <tt>#</tt>:</p>

<code># new_state filename timestamp</code>

<p>Each file state line records the state change and time:</p>

<ul>
	<li><b>new_state</b> - the integer represents the new state this file has just entered. The value of the integer ranges from 0 to 4 and the states they are representing are:</li>
	<ol start="0">
		<li> unknown </li>
		<li> expect </li>
		<li> exists </li>
		<li> complete </li>
		<li> delete </li>
	</ol>
	<li><b>filename</b> - the file name given in the rule specification of Makeflow.</li>
	<li><b>timestamp</b> - the unix time (in microseconds) when this line is written to the log file. </li>
</ul>

</div>
</body>
</html>
