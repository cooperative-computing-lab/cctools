<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>

<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8">
<link rel="stylesheet" type="text/css" href="manual.css">
<title>Makeflow User's Manual</title>
</head>

<body>

<div id="manual">
<h1>Makeflow User's Manual</h1>

<p style="text-align: right;"><b>Last edited: December 2014</b></p>

<p>Makeflow is Copyright (C) 2009 The University of Notre Dame.<br>
This software is distributed under the GNU General Public License.<br>
See the file COPYING for details.</p>

<h2 id="overview">Overview<a class="sectionlink" href="#overview" title="Link to this section.">&#x21d7;</a></h2>

<p>Makeflow is a <b>workflow engine</b> for distributed computing.  It accepts
a specification of a large amount of work to be performed, and runs it on
remote machines in parallel where possible.  In addition, Makeflow is
fault-tolerant, so you can use it to coordinate very large tasks that may run
for days or weeks in the face of failures.  Makeflow is designed to be similar
to <b>Make</b>, so if you can write a Makefile, then you can write a
Makeflow.</p>

<p>You can run a Makeflow on your local machine to test it out.  If you have a
multi-core machine, then you can run multiple tasks simultaneously.  If you
have a cluster or a batch system, then you can send your
jobs there to run.  If you don't already have a batch system, Makeflow comes
with a system called Work Queue that will let you distribute the load across
any collection of machines, large or small.</p>

<p>Makeflow currently supports the following batch systems:
<table>
<tr><td>local    <td> Local execution on the submitting machine, using multiple cores if available.
<tr><td>wq       <td> <a href=http://ccl.cse.nd.edu/software/workqueue>Work Queue</a> distributed application system, included with Makeflow.
<tr><td>condor   <td> <a href=http://www.cs.wisc.edu/htcondor>HTCondor</a> distributed computing system.
<tr><td>sge      <td> <a href=http://gridscheduler.sourceforge.net>Open Grid Scheduler</a>, Univa Grid Engine, Oracle Grid Engine, all derived from Sun Grid Engine (SGE).
<tr><td>pbs      <td> <a href=http://www.arc.ox.ac.uk/content/pbs>Portable Batch Scheduler</a>
<tr><td>torque   <td> <a href=http://www.adaptivecomputing.com/products/open-source/torque>Torque Resource Manager</a>
<tr><td>slurm    <td> <a href=http://slurm.schedmd.com>Slurm Workload Manager</a>
<tr><td>moab     <td> <a href="http://www.adaptivecomputing.com/resources/docs/mwm/6-0/index.php">Moab Workload Manager</a></li>
<tr><td>cluster  <td> Custom-defined batch submission commands, see <a href=#custom.drivers>custom drivers</a> below for details.
<tr><td>chirp    <td> Active storage jobs on the <a href=http://ccl.cse.nd.edu/software/chirp>Chirp filesystem</a>.
</table>
</p>

<p> Makeflow is part of the <a
href="http://ccl.cse.nd.edu/software">Cooperating Computing Tools</a>.  You can
download the CCTools from <a
href="http://ccl.cse.nd.edu/software/download">this web page</a>, follow the <a
href="install.html">installation instructions</a>, and you are ready to go.</p>

<h2 id="language">The Makeflow Language<a class="sectionlink" href="#language" title="Link to this section.">&#x21d7;</a></h2>

<p>The Makeflow language is very similar to Make.  A Makeflow script consists
of a set of rules.  Each rule specifies a set of <i>target files</i> to create,
a set of <i>source files</i> needed to create them, and a <i>command</i> that
generates the target files from the source files.</p>

<p>Makeflow attempts to generate all of the target files in a script.  It
examines all of the rules and determines which rules must run before others.
Where possible, it runs commands in parallel to reduce the execution time.</p>

<p>Here is a Makeflow that uses the <tt>convert</tt> utility to make an
animation.  It downloads an image from the web, creates four variations of the
image, and then combines them back together into an animation.  The first and
the last task are marked as LOCAL to force them to run on the controlling
machine.</p>

<code>CURL=/usr/bin/curl
CONVERT=/usr/bin/convert
URL=http://ccl.cse.nd.edu/images/capitol.jpg

capitol.montage.gif: capitol.jpg capitol.90.jpg capitol.180.jpg capitol.270.jpg capitol.360.jpg
        LOCAL $CONVERT -delay 10 -loop 0 capitol.jpg capitol.90.jpg capitol.180.jpg capitol.270.jpg capitol.360.jpg capitol.270.jpg capitol.180.jpg capitol.90.jpg capitol.montage.gif

capitol.90.jpg: capitol.jpg $CONVERT
        $CONVERT -swirl 90 capitol.jpg capitol.90.jpg

capitol.180.jpg: capitol.jpg $CONVERT
        $CONVERT -swirl 180 capitol.jpg capitol.180.jpg

capitol.270.jpg: capitol.jpg $CONVERT
        $CONVERT -swirl 270 capitol.jpg capitol.270.jpg

capitol.360.jpg: capitol.jpg $CONVERT
        $CONVERT -swirl 360 capitol.jpg capitol.360.jpg

capitol.jpg: $CURL
        LOCAL $CURL -o capitol.jpg $URL
</code>

<p>Note that Makeflow differs from Make in a few important ways.
The <a href=#details>complete details</a> are given below.</p>

<h2 id="running">Running Makeflow<a class="sectionlink" href="#running" title="Link to this section.">&#x21d7;</a></h2>

<p>To try out the example above, copy and paste it into a file named
<tt>example.makeflow</tt>.  To run it on your local machine:</p>

<code>makeflow example.makeflow</code>

<p>Note that if you run it a second time, nothing will happen, because all of
the files are built:</p>

<code>makeflow example.makeflow
makeflow: nothing left to do
</code>

<p>Use the <tt>-c</tt> option to clean everything up before trying it
again:</p>

<code>makeflow -c example.makeflow</code>

<p>If you have access to a batch system like Condor, SGE, Torque, SLURM, or Moab,
you can direct Makeflow to run your jobs there:</p>

<code>
makeflow -T condor example.makeflow
# or:
makeflow -T sge example.makeflow
# or:
makeflow -T torque example.makeflow
...
</code>

<h2 id="workqueue">Running Makeflow with Work Queue<a class="sectionlink" href="#workqueue" title="Link to this section.">&#x21d7;</a></h2>

<p>You will notice that a workflow can run very slowly if you submit each task
in the usual way, because it typically takes 30 seconds or so to start each
batch job running.  To get around this limitation, we provide the Work Queue
system.  This allows Makeflow to function as a master process that quickly
dispatches work to remote worker processes.</p>

<p>To begin, let's assume that you are logged into a machine named
<tt>barney.nd.edu</tt>. start your Makeflow like this:</p>

<code>makeflow -T wq example.makeflow</code>

<p>Then, submit 10 worker processes to Condor like this:</p>

<code>condor_submit_workers barney.nd.edu 9123 10
Submitting job(s)..........
Logging submit event(s)..........
10 job(s) submitted to cluster 298.
</code>

<p>Or, submit 10 worker processes to SGE like this:</p>

<code>sge_submit_workers barney.nd.edu 9123 10</code>

<p>Or, you can start workers manually on any other machine you can log into:</p>

<code>work_queue_worker barney.nd.edu 9123</code>

<p>Once the workers begin running, Makeflow will dispatch multiple tasks to
each one very quickly.  If a worker should fail, Makeflow will retry the work
elsewhere, so it is safe to submit many workers to an unreliable system.</p>

<p>When the Makeflow completes, your workers will still be available, so you
can either run another Makeflow with the same workers, remove them from the
batch system, or wait for them to expire.  If you do nothing for 15 minutes,
they will automatically exit.</p>

<p>Note that <tt>condor_submit_workers</tt> and <tt>sge_submit_workers</tt>,
are simple shell scripts, so you can edit them directly if you would like to
change batch options or other details.</p>

<h3 id="workqueue.port">Port Numbers<a class="sectionlink" href="#workqueue.port" title="Link to this section.">&#x21d7;</a></h3>

<p>Makeflow listens on a port which the remote workers would connect to.  The
default port number is 9123.  Sometimes, however, the port number might be not
available on your system. You can change the default port via the <tt>-p
&lt;port number&gt;</tt> option. For example, if you want the master to listen
on port 9567 by default, you can run the following command:</p>

<code>makeflow -T wq -p 9567 example.makeflow</code>

<h3 id="workqueue.projectnames">Project Names<a class="sectionlink" href="#workqueue.projectnames" title="Link to this section.">&#x21d7;</a></h3>

<p>A simpler way to match workers to masters is to use the project name
matching. You can give the master a project name with the -N option.</p>

<code>makeflow -T wq -N MyProj example.makeflow </code>

<p>The -N option gives the master a project name called 'MyProj',
and will cause it to advertise its information
such as the project name, running status, hostname and port number, to a catalog server.
Then a worker can simply identify the workload by its project name.
By default, makeflow will use the global catalog server at <a href=http://catalog.cse.nd.edu>catalog.cse.nd.edu</a>
but this can be changed, as described below.
</p>

<p>To start a worker that automatically finds MyProj's master via the default
Notre Dame catalog server:</p>

<code>work_queue_worker -N MyProj </code>

<p>You can also give multiple -N options to a worker. The worker will find out
which ones of the specified projects are running from the catalog server and
randomly select one to work for. When one project is done, the worker would
repeat this process.  Thus, the worker can work for a different master without
being stopped and given the different master's hostname and port. An example of
specifying multiple projects:</p>

<code>work_queue_worker -N proj1 -N proj2 -N proj3</code>

<h3 id="workqueue.password">Setting a Password<a class="sectionlink" href="#workqueue.password" title="Link to this section.">&#x21d7;</a></h3>

<p>We recommend that any workflow that uses a project name also set a password.
To do this, select any password and write it to a file called
<tt>mypwfile</tt>.  Then, run Makeflow and each worker with the
<tt>--password</tt> option to indicate the password file:</p>

<code>makeflow <b>--password</b> mypwfile ...
work_queue_worker <b>--password</b> mypwfile ...
</code>

<h3 id="workqueue.catalog">Catalog Server<a class="sectionlink" href="#workqueue.catalog" title="Link to this section.">&#x21d7;</a></h3>

<p>Now let's look at how to set up your own catalog server. Say you want to
run your catalog server on a machine named <tt>catalog.somewhere.edu</tt>.
The default port that the catalog server will be listening on is 9097, you can
change it via the '-p' option.</p>

<code>catalog_server</code>

<p>Now you have a catalog server listening at catalog.somewhere.edu:9097. To
make your masters and workers contact this catalog server, simply add the
<tt>-C hostname:port</tt> option to both of your master and worker:</p>

<code>makeflow -T wq -C catalog.somewhere.edu:9097 -N MyProj example.makeflow
work_queue_worker -C catalog.somewhere.edu:9097 -a -N MyProj
</code>

<h2 id="details">More Language Details<a class="sectionlink" href="#details" title="Link to this section.">&#x21d7;</a></h2>

<p>The Makeflow language is very similar to Make, but it does have a few important differences that you should be aware of.</p>

<h3 id="details.dependencies">Get the Dependencies Right<a class="sectionlink" href="#details.dependencies" title="Link to this section.">&#x21d7;</a></h3>

<p>You must be careful to accurately specify <b>all of the files that a rule
requires and creates</b>, including any custom executables.  This is because
Makeflow requires all these information to construct the environment for a
remote job.  For example, suppose that you have written a simulation program
called <tt>mysim.exe</tt> that reads <tt>calib.data</tt> and then produces and
output file.  The following rule won't work, because it doesn't inform Makeflow
what files are neded to execute the simulation:</p>

<code># This is an incorrect rule.

output.txt:
        ./mysim.exe -c calib.data -o output.txt
</code>

<p>However, the following is correct, because the rule states all of the files
needed to run the simulation.  Makeflow will use this information to construct
a batch job that consists of <tt>mysim.exe</tt> and <tt>calib.data</tt> and
uses it to produce <tt>output.txt</tt>:</p>

<code># This is a correct rule.

output.txt: mysim.exe calib.data
        ./mysim.exe -c calib.data -o output.txt
</code>

<p>Note that when a directory is specified as an input dependency, it
means that the command relies on the directory and all of its contents.
So, if you have a large collection of input data, you can place it
in a single directory, and then simply give the name of that directory.
</p>

<h3 id="details.phony">No Phony Rules<a class="sectionlink" href="#details.phony" title="Link to this section.">&#x21d7;</a></h3>

<p>For a similar reason, you cannot have "phony" rules that don't actually
create the specified files.  For example, it is common practice to define a
<tt>clean</tt> rule in Make that deletes all derived files.  This doesn't make
sense in Makeflow, because such a rule does not actually create a file named
<tt>clean</tt>.  Instead use the <tt>-c</tt> option as shown above.</p>

<h3 id="details.plain">Just Plain Rules<a class="sectionlink" href="#details.plain" title="Link to this section.">&#x21d7;</a></h3>

<p>Makeflow does not support all of the syntax that you find in various
versions of Make.  Each rule must have exactly one command to execute.  If you
have multiple commands, simply join them together with semicolons.  Makeflow
allows you to define and use variables, but it does not support  pattern rules,
wildcards, or special variables like <tt>$&lt;</tt> or <tt>$@</tt>.  You simply
have to write out the rules longhand, or write a script in your favorite
language to generate a large Makeflow.</p>

<h3 id="details.local">Local Job Execution<a class="sectionlink" href="#details.local" title="Link to this section.">&#x21d7;</a></h3>

<p>Certain jobs don't make much sense to distribute.  For example, if you have
a very fast running job that consumes a large amount of data, then it should
simply run on the same machine as Makeflow.  To force this, simply add the word
<tt>LOCAL</tt> to the beginning of the command line in the rule.</p>

<h3 id="details.scope">Rule Lexical Scope<a class="sectionlink" href="#details.scope" title="Link to this section.">&#x21d7;</a></h3>

<p>Variables in Makeflow have global scope, that is, once defined, their value
can be accessed from any rule. Sometimes it is useful to define a variable
locally inside a rule, without affecting the global value. In Makeflow, this
can be achieved by defining the variables after the rule's requirements, but
before the rule's command, and prepending the name of the variable with
<tt>@</tt>, as follows:</p>

<code>SOME_VARIABLE=original_value

target_1: source_1
	command_1

target_2: source_2
@SOME_VARIABLE=local_value_for_2
	command_2

target_3: source_3
	command_3 </code>

In this example, SOME_VARIABLE has the value 'original_value' for rules 1 and
3, and the value 'local_value_for_2' for rule 2.

<h3 id="details.environment">Environment Variables<a class="sectionlink" href="#details.environment" title="Link to this section.">&#x21d7;</a></h3>

Environment variables can be defined with the <tt>export</tt> keyword inside a workflow.  Makeflow will communicate explicitly named environment variables
to remote batch systems, where they will override whatever local setting is present.  For example, suppose you want to modify the PATH for every job in the makeflow:

<code>export PATH=/opt/tools/bin:${PATH}</code>

If no value is given, then the current value of the environment variable is passed along to the job:

<code>export USER</code>

<h3 id="details.refinement">Batch Job Refinement<a class="sectionlink" href="#details.refinement" title="Link to this section.">&#x21d7;</a></h3>

<p>When executing jobs, Makeflow simply uses the default settings in your batch
system.  If you need to pass additional options, use the <tt>BATCH_OPTIONS</tt>
variable or the <tt>-B</tt> option to Makeflow.</p>

<p>When executing jobs, Makeflow simply uses the default settings in your batch
system.  If you need to pass additional options, use the <tt>BATCH_OPTIONS</tt>
variable or the <tt>-B</tt> option to Makeflow.</p>

<p>When using Condor, this string will be added to each submit file.  For
example, if you want to add <tt>Requirements</tt> and <tt>Rank</tt> lines to
your Condor submit files, add this to your Makeflow:</p>

<code>BATCH_OPTIONS = Requirements = (Memory&gt;1024)</code>
<p>
When using SGE, the string will be added to the qsub options.  For example, to specify that jobs should be submitted to the <tt>devel</tt> queue:
<code>BATCH_OPTIONS = -q devel</code>

<h3 id="details.renaming">Remote File Renaming<a class="sectionlink" href="#details.renaming" title="Link to this section.">&#x21d7;</a></h3>

<p>With the Work Queue and Condor batch systems, Makeflow has a feature called
remote file renaming. For example:</p>

<code>local_name-&gt;remote_name</code>

<p>indicates that the file <tt>local_name</tt> is called <tt>remote_name</tt>
in the remote system. Consider the following example:</p>

<code>b.out: a.in myprog
LOCAL myprog a.in &gt; b.out

c.out-&gt;out: a.in-&gt;in1 b.out myprog-&gt;prog
        prog in1 b.out &gt; out
</code>

<p>The first rule runs locally, using the executable <tt>myprog</tt> and the
local file <tt>a.in</tt> to locally create <tt>b.out</tt>.  The second rule
runs remotely, but the remote system expects <tt>a.in</tt> to be named
<tt>in1</tt>, <tt>c.out</tt>, to be named <tt>out</tt> and so on. Note that we
did not need to rename the file <tt>b.out</tt>. Without remote file renaming,
we would have to create either a symbolic link, or a copy of the files with the
expected correct names.</p>

<h2 id="details.displaying">Displaying a Makeflow<a class="sectionlink" href="#details.displaying" title="Link to this section.">&#x21d7;</a></h2>

<p>There are several ways to visualize both the structure of a Makeflow
as well as its progress over time.
<tt>makeflow_viz</tt> can be used to convert a Makeflow into
a file that can be displayed by <a href="http://www.graphviz.org">Graphviz DOT</a> tools like this:</p>

<code>makeflow_viz -D dot example.makeflow &gt; example.dot<br>
dot -Tgif &lt; example.dot &gt; example.gif
</code>
<p>
Or use a similar command to generate a <a href="http://www.cytoscape.org">Cytoscape</a> input file.  (This will also create a Cytoscape <tt>style.xml</tt> file.)</p>

<code>makeflow_viz -D cyto example.makeflow > example.xgmml</code>

<p>To observe how a makeflow runs over time, use <tt>makeflow_graph_log</tt> to convert a log file into a timeline that shows the number of tasks ready, running, and complete over time:</p>

<code>makeflow_graph_log example.makeflowlog example.png</code>

<h2 id="wrappers">Wrapper Commands<a class="sectionlink" href="#wrappers" title="Link to this section.">&#x21d7;</a></h2>

<p>
Makeflow allows a global wrapper command to be applied to every rule in the workflow.  This is particularly useful for applying troubleshooting tools, or for setting up a global environment without rewriting the entire workflow.  The <tt>--wrapper</tt> option will prefix a command in front of every rule, while the <tt>--wrapper-input</tt> and <tt>--wrapper-output</tt> options will specify input and output files related to the wrapper.</p>

<p>A few special characters are supported by wrappers.  If the wrapper command or wrapper files contain two percents (<tt>%%</tt>), then the number of the current rule will be substituted there.  If the command contains curly braces (<tt>{}</tt>) the original command will be substituted at that point, instead of at the end of the command.</p>

<p>For example, suppose that you wish to apply <tt>/usr/bin/time</tt> to every rule in the workflow.  Instead of modifying the workflow, run it like this:</p>

<code>makeflow --wrapper '/usr/bin/time -p' example.makeflow</code>

<p>Suppose you want to apply <tt>strace</tt> to every rule, to obtain system call traces.  Since every rule would have to have its own output file for the trace, you could indicate output files like this:</p>

<code>makeflow --wrapper 'strace -o trace.%%' --wrapper-output 'trace.%%' example.makeflow</code>

<p>Suppose you want to wrap every command with a script that would set up an appropriate Java environment.  You might write a script called <tt>setupjava.sh</tt> like this:</p>

<code>#!/bin/sh<br>
export JAVA_HOME=/opt/java-9.8.3.6.7<br>
export PATH=${JAVA_HOME}/bin:$PATH<br>
echo "using java in $JAVA_HOME"<br>
exec "$@"<br>
</code>

<p>And then invoke Makeflow like this:</p>

<code>makeflow --wrapper ./setupjava.sh --wrapper-input setupjava.sh example.makeflow</code>


<h2 id="resources">Resources and Categories<a class="sectionlink" href="#resources" title="Link to this section.">&#x21d7;</a></h2>

<p>
Makeflow has the capability of automatically setting the cores, memory,
and disk space requirements to the underlying batch system (currently this only
works with Work Queue and Condor). Jobs are grouped into <em> job categories
</em>, and jobs in the same category have the same cores, memory, and disk
requirements.
</p>
<p>
Job categories and resources are specified with variables.  Jobs are assigned
to the category named in the value of the variable CATEGORY.  Likewise, the
values of the variables CORES, MEMORY (in MB), and DISK (in MB) describe the
resources requirements for the category specified in CATEGORY.
</p>
<p>
Jobs without an explicit category are assigned to <tt>default</tt>.  Jobs in
the default category get their resource requirements from the value of the
environment variables CORES, MEMORY, and DISK.
</p>

Consider the following example:

<code> # These tasks are assigned to the category preprocessing.
# MEMORY and CORES are read from the environment, if defined.

CATEGORY="preprocessing"
DISK=500

one: src
    cmd

two: src
    cmd

# These tasks have the category "simulation". Note that now CORES, MEMORY, and DISK are specified.

CATEGORY="simulation"
CORES=1
MEMORY=400
DISK=400

three: src
    cmd

four: src
    cmd

# Another category switch. MEMORY is read from the environment.

CATEGORY="analysis"
CORES=4
DISK=600

five: src
    cmd
</code>

<code>export MEMORY=800
makeflow ...
</code>

<h6> Resources specified: </h6>
<table>
<tr><th>Category</th><th>Cores</th><th>Memory (MB)</th><th>Disk (MB)</th></tr>
<tr><td> preprocessing </td> <td> (unspecified) </td> <td> 800 (from environment) </td> <td> 500 </td> </tr>
<tr><td> simulation </td> <td> 1 </td> <td> 400 </td> <td> 400 </td> </tr>
<tr><td> analysis </td> <td> 4 </td> <td> 800  (from environment)</td> <td> 600 </td> </tr>
</table>

<h2 id="linking">Linking Dependencies<a class="sectionlink" href="#linking" title="Link to this section.">&#x21d7;</a></h2>

<p><tt>Makeflow</tt> provides a tool to collect all of the dependencies for a
given workflow into one directory. By collecting all of the input files and
programs contained in a workflow it is possible to run the workflow on other
machines.</p>

<p>Currently, <tt>Makeflow</tt> copies all of the files specified as
dependencies by the rules in the makeflow file, including scripts and data
files. Some of the files not collected are dynamically linked libraries,
executables not listed as dependencies (<tt>python</tt>, <tt>perl</tt>), and
configuration files (<tt>mail.rc</tt>).</p>

<p>To avoid naming conflicts, files which would otherwise have an identical
path are renamed when copied into the bundle:</p>

<ul>
  <li>All file paths are relative to the top level directory.</li>
  <li>The makeflow file is rewritten with the new file locations and placed in the top level directory.</li>
  <li>Files originally specified with an absolute path are placed into the top level directory.</li>
  <li>Files with the same path are appended with a unique number</li>
</ul>

<p>Example usage:</p>

<code>makeflow -b some_output_directory example.makeflow</code>

<h2 id="garbage">Garbage Collection<a class="sectionlink" href="#garbage" title="Link to this section.">&#x21d7;</a></h2>

<p>As the workflow execution progresses, Makeflow can automatically delete
intermediate files that are no longer needed. In this context, an intermediate
file is an input of some rule that is the target of another rule. Therefore, by
default, garbage collection does not delete the original input files, nor <b>
final</b> target files.</p>

<p>Which files are deleted can be tailored from the default by appending files
to the Makeflow variables <tt>GC_PRESERVE_LIST</tt> and
<tt>GC_COLLECT_LIST</tt>.  Files added to <tt>GC_PRESERVE_LIST</tt> are never
deleted, thus it is used to mark intermediate files that should not be deleted.
Similarly, <tt>GC_COLLECT_LIST</tt> marks final target files that should be
deleted.  Makeflow is conservative, in the sense that <tt>GC_PRESERVE_LIST</tt>
takes precedence over <tt>GC_COLLECT_LIST</tt>, and original input files are
never deleted, even if they are listed in <tt>GC_COLLECT_LIST</tt>.</p>

<p>Makeflow offers two modes for garbage collection: reference count, and on
demand. With the reference count mode, intermediate files are deleted as soon
as no rule has them listed as input. The on-demand mode is similar to reference
count, only that files are deleted until the space on the local file system is
below a given threshold.</p>

<p>To activate reference count garbage collection:</p>

<code>makeflow -gref_count</code>

<p>To activate on-demand garbage collection, with a threshold of 500MB:</p>

<code>makeflow -gon_demand -G500000000</code>

<h2 id="logformat">Log File Format<a class="sectionlink" href="#logformat" title="Link to this section.">&#x21d7;</a></h2>

<p>After you  have executed the <tt>example.makeflow</tt> Makeflow script, you
should see a log file named <tt>example.makeflow.makeflowlog</tt> under the
directory where you ran the <tt>makeflow</tt> command.  The Makeflow log file
records how and when every task is run by Makeflow.  It exists primarily so
that Makeflow can recover cleanly after a failure, but can also be used for logging
and debugging.</p>

<p>A sample logfile might look like this:</p>

<code># STARTED timestamp
1347281321284638 5 1 9206 5 1 0 0 0 6
1347281321348488 5 2 9206 5 0 1 0 0 6
1347281321348760 4 1 9207 4 1 1 0 0 6
1347281321348958 3 1 9208 3 2 1 0 0 6
1347281321629802 4 2 9207 3 1 2 0 0 6
1347281321630005 2 1 9211 2 2 2 0 0 6
1347281321635236 3 2 9208 2 1 3 0 0 6
1347281321635463 1 1 9212 1 2 3 0 0 6
1347281321742870 2 2 9211 1 1 4 0 0 6
1347281321752857 1 2 9212 1 0 5 0 0 6
1347281321753064 0 1 9215 0 1 5 0 0 6
1347281325731146 0 2 9215 0 0 6 0 0 6
# COMPLETED timestamp
</code>

<p>
Each line in the log file represents a single action taken on a single rule in the workflow.
For simplicity, rules are numbered from the beginning of the Makeflow, starting with zero.
Each line contains the following items:
</p>

<code>timestamp task_id new_state job_id tasks_waiting tasks_running tasks_complete tasks_failed tasks_aborted task_id_counter</code>

<p>Which are defined as follows:</p>

<ul>
	<li><b>timestamp</b> - the unix time (in microseconds) when this line is written to the log file. </li>
	<li><b>task_id</b> - the id of this n. </li>
	<li><b>new_state</b> - a integer represents the new state this task (whose id is in the task_id column) has just entered. The value of the integer ranges from 0 to 4 and the states they are representing are:</li>
	<ol start="0">
		<li> waiting </li>
		<li> running </li>
		<li> complete </li>
		<li> failed </li>
		<li> aborted </li>
	</ol>
	<li><b>job_id</b> - the job id of this task in the underline execution
	system (local or batch system). If the Makeflow is executed locally, the
	job id would be the process id of the process that executes this task. If
	the underline execution system is a batch system, such as Condor or SGE,
	the job id would be the job id assigned by the batch system when the task
	was sent to the batch system for execution.</li>
	<li><b>tasks_waiting</b> - the number of tasks are waiting to be executed.</li>
	<li><b>tasks_running</b> - the number of tasks are being executed.</li>
	<li><b>tasks_complete</b> - the number of tasks has been completed.</li>
	<li><b>tasks_failed</b> - the number of tasks has failed.</li>
	<li><b>tasks_aborted</b> - the number of tasks has been aborted.</li>
	<li><b>task_id_counter</b> - total number of tasks in this Makeflow</li>
</ul>

<p> In addition, lines starting with a pound sign are comments and
contain additional high-level information that can be safely ignored.
The logfile begins with a comment to indicate the starting time, and ends with
a comment indicating whether the entire workflow completed, failed, or was aborted.
</p>

<p>When file garbage collection is enabled, the log file also records information at
each collection cycle. Collection information is included in lines starting
with the <tt># GC</tt> prefix:</p>

<code># GC timestamp collected time_spent dag_gc_collected</code>

<p>Each garbage collection line records the garbage collection statistics
during a garbage collection cycle:</p>

<ul>
	<li><b>timestamp</b> - the unix time (in microseconds) when this line is written to the log file. </li>
	<li><b>collected</b> - the number of files were collected in this garbage collection cycle.</li>
	<li><b>time_spent</b> - the length of time this cycle took.</li>
	<li><b>dag_gc_collected</b> - the total number of files has been collected so far since the start this Makeflow execution.</li>
</ul>

<h2 id="custom.drivers">Custom Drivers<a class="sectionlink" href="#custom.drivers" title="Link to this section.">&#x21d7;</a></h2>

<p>For clusters that are not directly supported by Makeflow we strongly suggest
using the <a href="http://ccl.cse.nd.edu/software/workqueue">Work Queue</a> system
and submitting workers via the cluster's normal submission mechanism.</p>

<p>For clusters using managers similar to Torque, SGE, and PBS that submit
jobs with commands like "qsub", you can inform makeflow of those commands
and use the <tt>cluster</tt> driver.  For this to work, it is assumed
there is a distributed filesystem shared (like NFS) shared across all
nodes of the cluster.</p>

<p>To configure a custom driver, set the following environment variables:

<ul>
	<li>BATCH_QUEUE_CLUSTER_NAME: The name of the cluster, used in debugging messages and as the name for the wrapper script.</li>
	<li>BATCH_QUEUE_CLUSTER_SUBMIT_COMMAND: The submit command for the cluster (such as qsub or msub)</li>
	<li>BATCH_QUEUE_CLUSTER_SUBMIT_OPTIONS: The command-line arguments that must be passed to the submit command.  This string should end with the argument used to set the name of the task (usually -N).</li>
	<li>BATCH_QUEUE_CLUSTER_REMOVE_COMMAND: The delete command for the cluster (such as qdel or mdel)</li>
</ul>

<p>These will be used to construct a task submission for each makeflow rule that consists of:</p>

<code>$SUBMIT_COMMAND $SUBMIT_OPTIONS $CLUSTER_NAME.wrapper "&lt;rule commandline&gt;"</code>

<p>The wrapper script is a shell script that reads the command to be run as an argument and handles bookkeeping operations necessary for Makeflow.</p>

<h2 id="moreinfo">For More Information<a class="sectionlink" href="#moreinfo" title="Link to this section.">&#x21d7;</a></h2>

<p>For the latest information about Makeflow, please visit our <a href="http://ccl.cse.nd.edu/software/makeflow">web site</a> and subscribe to our <a href="http://ccl.cse.nd.edu/software">mailing list</a>.</p>

</div>
</body>
</html>
