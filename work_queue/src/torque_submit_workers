#!/bin/sh

SUBMIT_COMMON=`which wq_submit_workers.common`
if [ -z "$SUBMIT_COMMON" ];
then
	echo "Please add 'wq_submit_workers.common' to your PATH."
else
	. $SUBMIT_COMMON
fi

show_help() 
{
	echo "  -j               Use job array to submit workers."
	echo "  -p <parameters>  Torque qsub parameters."
}

use_jobarray=0
torque_parameters=""

parse_arguments()
{

	# if [ -z "$cores" -o $cores=0 ]
	# then
	# 	cores=1
	# fi

	while [ $# -gt 0 ]
	do
		case $1 in
			-j)
			use_jobarray=1
			;;
			-p)
			shift
			torque_parameters="$torque_parameters $1"
			;;
			*)
			break
			;;
		esac
		shift
	done
}




submit_workers_command()
{

	qsub=`which qsub 2>/dev/null`
	if [ $? != 0 ]
	then
		echo "$0: please add 'qsub' to your PATH."
		exit 1
	fi

        #====================================#
        #| This script is the only one that |#
        #|      goes three layers deep.     |#
        #====================================#
        # First create the script that will
        # be submitted to the scheduler.
        cat >worker.sh <<EOF
#!/bin/bash
#PBS -N $jobname
#PBS -l nodes=$nodes:ppn=$(( cores * numworkers ))$node_extras
$qsub_extras

export PATH=$(dirname $(which qsub)):\$PATH

#===========================================#
#| If submitting a multiple-node job, then |#
#| the second-layer script is broadcast to |#
#| each allocated node using "pbsdsh".     |#
#===========================================#
. /etc/profile
. /etc/bashrc
. ~/.bashrc

if [[ \$HOSTNAME =~ "nid" ]] ; then
    module add ccm
    if [[ x$gpuq == x ]] ; then
        qchem42 --stage
    fi
    ccmlogin "$PWD/worker1.sh \$PBS_JOBID"
    sleep 999999
elif [ $nodes -gt 1 ] ; then
    pbsdsh -u -v $PWD/worker1.sh
else
    $PWD/worker1.sh
fi
EOF

        #====================================================#
        #| Create the second layer script.  This gives us   |#
        #| the option of whether we should SSH back into    |#
        #| the local node before running the worker, which  |#
        #| allows us to surpass some buggy resource limits. |#
        #====================================================#
        cat <<EOF > worker1.sh
#!/bin/bash

if [[ \$HOSTNAME =~ "nid" ]] ; then
    for i in \`uniq $HOME/.crayccm/ccm_nodelist.\$1 \` ; do
        nohup ssh \$i "$PWD/worker2.sh &> $PWD/worker.log.\\\$HOSTNAME &"
    done
elif [ "x$self_ssh" != "x" ] ; then
    ssh \$HOSTNAME "$PWD/worker2.sh \$\$"
else
    $PWD/worker2.sh
fi
wait
EOF

        #=======================================#
        #|    Create the third layer script.   |#
        #| This actually launches the workers. |#
        #=======================================#
        cat <<EOF > worker2.sh
#!/bin/bash

# Load environment variables.
. /etc/profile
. /etc/bashrc
. $HOME/.bashrc

# Limit core dump size.
ulimit -c 0

# This function makes the script kill itself if:
# 1) the second layer stops running (i.e. job deleted by scheduler)
# 2) there are no more workers (i.e. idle timeout)
waitkill(){
    while sleep 1 ; do 
        kill -0 \$1 2> /dev/null || break
        [ \$( ps xjf | grep work_queue_worker | grep -v grep | wc -l ) -gt 0 ] || break
    done
    kill -TERM -\$\$
};

# Go into the directory where the worker program is.
cd $PWD

# Set environment variables.
export OMP_NUM_THREADS=$cores
export MKL_NUM_THREADS=$cores
export _CONDOR_SCRATCH_DIR=$scratch_dir
mkdir -p \$_CONDOR_SCRATCH_DIR

# Create the PBS Node File
if [ "x$NODEFILE_DIR" != "x" ] ; then
    export PBS_NODEFILE=$NODEFILE_DIR/pbs_nodefile.\$HOSTNAME
    rm -f \$PBS_NODEFILE
    for i in \`seq $(( cores * numworkers ))\` ; do
        echo \$HOSTNAME >> \$PBS_NODEFILE
    done
fi

# Optional SSH port forwarding
if [ $forward == 1 ]; then
    if [ \`ps aux | grep $USER | grep $headnode | grep ServerAlive | grep $port | grep -v grep | awk '{print \$2}' | wc -l\` -eq 0 ] ; then
        if [[ \$HOSTNAME =~ "nid" ]] ; then
            # On Blue Waters the SSH port forwarding connection is compute node --> not0rious --> master
            ssh -p 22 -x -o ServerAliveInterval=180 -N -f -L$port:$host:$port not0rious.stanford.edu
        else
            ssh -x -o ServerAliveInterval=180 -N -f -L$port:localhost:$port $headnode
        fi
    fi
fi

if [[ "x$self_ssh" != "x" ]] ; then
    waitkill \$1 &
fi

# Actually execute the workers.
for i in \`seq $numworkers\`; do
    export CUDA_DEVICE=\$(( i - 1 ))
    if [ $forward == 1 ]; then
        ./work_queue_worker -d all $arguments localhost $port &
    else
        ./work_queue_worker -d all $arguments $host $port &
    fi
done

wait
EOF

	chmod 755 worker*.sh

	if [ $use_jobarray = 1 ]
	then
		qsub -t 1-$count:1 -d `pwd` $torque_parameters worker.sh	
	else 
		for n in `seq 1 $count`
		do
			qsub -d `pwd` $torque_parameters worker.sh
		done
	fi
}

submit_workers "$@"
