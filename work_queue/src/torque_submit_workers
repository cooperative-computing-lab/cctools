#!/bin/sh

show_help() 
{
	echo "Use: torque_submit_workers [options] <servername> <port> <num-workers>"
	echo "where options are:"
	echo "  -M <name>        Name of the preferred master for worker."
	echo "  -N <name>        Same as -M (backwards compatibility)."
	echo "  -c <num>         Set the number of cores each worker should use (0=auto). (default=1)"
	echo "  -C <catalog>     Set catalog server to <catalog>. <catalog> format: HOSTNAME:PORT."
	echo "  -t <time>        Abort after this amount of idle time. (default=900s)."
	echo "  -d <subsystem>   Enable debugging on worker for this subsystem (try -d all to start)."
	echo "  -w <size>        Set TCP window size."
	echo "  -i <time>        Set initial value for backoff interval when worker fails to connect to a master. (default=1s)"
	echo "  -b <time>        Set maxmimum value for backoff interval when worker fails to connect to a master. (default=60s)"
	echo "  -z <size>        Set available disk space threshold (in MB). When exceeded worker will clean up and reconnect. (default=100MB)"
	echo "  -A <arch>        Set architecture string for the worker to report to master instead of the value in uname."
	echo "  -O <os>          Set operating system string for the worker to report to master instead of the value in uname."
	echo "  -s <path>        Set the location for creating the working directory of the worker."
	echo "  -j               Use job array to submit workers."
	echo "  -f               SSH port forwarding enabled."
	echo "  -n <nodes>       Number of compute nodes that this job will reserve."
	echo "  -p <parameters>  Torque qsub parameters."
	echo "  -h               Show this help message."
	exit 1
}

# An tutorial for submitting workers with SSH port forwarding to work around firewalls, 
# compute nodes without Internet connectivity, and similar situations.
# --> denotes a command that you need to run.
# The master is workstation.university.edu at port 9999.  
# The head node is headnode.cluster.com.
# 
# Consider a case where workstation.university.edu can connect to headnode.cluster.com, but not vice versa.
# On workstation.university.edu, I run the command:
# --> ssh -o ServerAliveInterval=180 -N -f -R9998:workstation.university.edu:9999 headnode.cluster.com
# This redirects host "localhost" port 9998 on headnode.cluster.com to host 
# "workstation.university.edu" port 9999 (numbers chosen to be different for clarity).
# 
# Now I submit a worker with the command:
# --> [pbs|torque|sge]_submit_workers -f headnode.cluster.com 9998
# The worker process will run the following command before starting the worker:
# ssh -o ServerAliveInterval=180 -N -f -L9998:localhost:9998 headnode.cluster.com
# This redirects host "localhost" port 9998 on the compute node to "localhost" port 9998 on headnode.cluster.com,
# which *then* gets redirected to host "workstation.univeresity.edu" port 9999.
# The worker should then connect to the master successfully..
# 
# Now consider where headnode.cluster.com can connect to workstation.university.edu but not vice versa.
# Repeat the above command, but on headnode.cluster.com I run the command:
# --> ssh -o ServerAliveInterval=180 -N -f -L9998:workstation.university.edu:9999 workstation.university.edu
# This redirects host "localhost" port 9998 on headnode.cluster.com to host "workstation.university.edu" port 9999.
# Note now -L creates a "localhost" port LOCALLY while -R creates a "localhost" port REMOTELY.

arguments=""
use_auto=0
use_jobarray=0
parameters=""
port_forward=0
nodes=1
cores=1

while getopts aM:N:c:C:t:d:w:i:b:z:A:O:s:fn:jp:h opt 
do
	case "$opt" in
		a)  arguments="$arguments -a"; use_auto=1;; #left here for backwards compatibility
		M)  arguments="$arguments -M $OPTARG"; use_auto=1;;
		N)  arguments="$arguments -M $OPTARG"; use_auto=1;;
		c)  arguments="$arguments --cores $OPTARG"; cores=$OPTARG;;
		C)  arguments="$arguments -C $OPTARG";;
		t)  arguments="$arguments -t $OPTARG";;
		d)  arguments="$arguments -d $OPTARG";;
		w)  arguments="$arguments -w $OPTARG";;
		i)  arguments="$arguments -i $OPTARG";;
		b)  arguments="$arguments -b $OPTARG";;
		z)  arguments="$arguments -z $OPTARG";;
		A)  arguments="$arguments -A $OPTARG";;
		O)  arguments="$arguments -O $OPTARG";;
		s)  arguments="$arguments -s $OPTARG";;
		j)  use_jobarray=1;;
        f)  port_forward=1;;
        n)  nodes=$OPTARG;;
		p)  parameters="$parameters $OPTARG";;
		h)  show_help;;
		\?) show_help;;
	esac
done

shift $(expr $OPTIND - 1)

if [ $use_auto = 0 ]; then
    if [ X$3 = X ]
    then
	show_help	
    fi
    host=$1
    port=$2
    count=$3
else
    if [ X$1 = X ]
    then
	show_help	
    fi
    host=
    port=
    count=$1
fi

worker=`which work_queue_worker 2>/dev/null`
if [ $? != 0 ]
then
	echo "$0: please add 'work_queue_worker' to your PATH."
	exit 1
fi

qsub=`which qsub 2>/dev/null`
if [ $? != 0 ]
then
	echo "$0: please add 'qsub' to your PATH."
	exit 1
fi

mkdir -p ${USER}-workers
cd ${USER}-workers
cp $worker .

cat >worker.sh <<EOF
#!/bin/sh
#PBS -l nodes=$nodes:ppn=$cores

# In order to enable a single batch job running on multiple nodes,
# the worker.sh script is split into two parts.
cd $PWD
if [ $numnodes -gt 1 ] ; then
    pbsdsh -u -v ./worker1.sh
else
    ./worker1.sh
fi
EOF

cat >worker1.sh <<EOF
#!/bin/bash
# The pbsdsh command doesn't load the user environment on each node
# so we manually load it here.
. /etc/profile
. /etc/bashrc
. /home/$USER/.bashrc

if [ $port_forward == 1 ]; then
    ssh -o ServerAliveInterval=180 -N -f -L$port:localhost:$port $host
    ./work_queue_worker $arguments localhost $port
else
    ./work_queue_worker $arguments $host $port
fi
EOF

chmod 755 worker.sh worker1.sh

if [ $use_jobarray = 1 ]
then
	qsub -t 1-$count:1 -d `pwd` $parameters worker.sh	
else 
	for n in `seq 1 $count`
	do
		qsub -d `pwd` $parameters worker.sh
	done
fi
return_status=$?

exit $return_status
